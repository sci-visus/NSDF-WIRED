{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca8ac1b-4eaa-478a-ad7e-1364e6c1791e",
   "metadata": {},
   "source": [
    "# Firesmoke Data Conversion to IDX using OpenVisus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f4506-47b9-452e-908e-15e532e1b801",
   "metadata": {},
   "source": [
    "## Import necessary libraries, install them if you do not have them. This was developed in Python 3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f499004e-bebd-4abf-8885-f16ce6ad9531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used to read/manipulate netCDF data\n",
    "import xarray as xr\n",
    "\n",
    "# Used to convert to .idx\n",
    "from OpenVisus import *\n",
    "\n",
    "# Used for numerical work\n",
    "import numpy as np\n",
    "\n",
    "# Used for processing netCDF time data\n",
    "import datetime\n",
    "\n",
    "# Used for interacting with OS file system (to get directory file names)\n",
    "import os\n",
    "\n",
    "# To load/save final sequence array to file\n",
    "import pickle\n",
    "\n",
    "# Used for resampling arrays to fit the same lat/lon grid\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# for checking and using timestamps\n",
    "import pandas as pd\n",
    "\n",
    "# Accessory, used to generate progress bar for running for loops\n",
    "# from tqdm.notebook import tqdm\n",
    "# import ipywidgets\n",
    "# import jupyterlab_widgets\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3169e-d1dd-4050-811a-6fd661442c81",
   "metadata": {},
   "source": [
    "## Get relevant directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b0c7861-34fb-464b-95b1-4a1aa4a8ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* THIS IS WHEN RUNNING FROM ATLANTIS.SCI **************\n",
    "firesmoke_dir = \"/usr/sci/cedmav/data/firesmoke\"\n",
    "\n",
    "# path to save idx file and data\n",
    "idx_dir = \"/usr/sci/scratch_nvme/arleth/idx/firesmoke\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d88f90-0370-481c-93be-e3c1e1a10ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata of datasets, had to be obtained manually\n",
    "ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\n",
    "start_dates = [\"20210304\", \"20210304\", \"20210304\", \"20210303\"]\n",
    "end_dates = [\"20240627\", \"20240627\", \"20240627\", \"20240627\"]\n",
    "\n",
    "id_dates = {ids[i]: {\"start_date\": start_dates[i], \"end_date\": end_dates[i]} for i in range(len(ids))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32528eee-cd60-44f6-9e14-cae79c5ce478",
   "metadata": {},
   "source": [
    "## Gather information about the metadata of our files, since it is inconsistent file to file. We need to know what to normalize across all files.\n",
    "\n",
    "### In particular:\n",
    "#### 1. Count number of files there are per firesmoke directory.\n",
    "#### 2. Determine maximum row,col dimension sizes for pm25 array.\n",
    "#### 3. Determine maximum latitude longitude grid parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c92053f-9e91-4e6d-b46b-a9efa04c6d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1023/1023 [00:12<00:00, 84.06it/s]\n",
      "100%|██████████| 1210/1210 [00:14<00:00, 86.40it/s]\n",
      "100%|██████████| 1022/1022 [00:11<00:00, 85.87it/s]\n",
      "100%|██████████| 1022/1022 [00:11<00:00, 86.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# List of all files that are available from UBC\n",
    "successful_files = {id_: [] for id_ in ids}\n",
    "\n",
    "# Variables to hold maxes, also to track the unique max values\n",
    "max_ncols = {id_: 0 for id_ in ids}\n",
    "max_nrows = {id_: 0 for id_ in ids}\n",
    "ncols = {id_: set() for id_ in ids}\n",
    "nrows = {id_: set() for id_ in ids}\n",
    "\n",
    "# Max grid dimensions\n",
    "max_grid_x = {id_: {\"xorig\": 0.0, \"xcell\": 0.0} for id_ in ids}\n",
    "max_grid_y = {id_: {\"yorig\": 0.0, \"ycell\": 0.0} for id_ in ids}\n",
    "xorigs = {id_: set() for id_ in ids}\n",
    "xcells = {id_: set() for id_ in ids}\n",
    "yorigs = {id_: set() for id_ in ids}\n",
    "ycells = {id_: set() for id_ in ids}\n",
    "\n",
    "for id_ in ids:\n",
    "    # get list of netcdf file names for each dataset\n",
    "    file_names = os.listdir(f'{firesmoke_dir}/{id_}/')\n",
    "    \n",
    "    # try opening each file, process only if it successfully opens\n",
    "    for file in tqdm(file_names):\n",
    "        # get file's path\n",
    "        path = f'{firesmoke_dir}/{id_}/{file}'\n",
    "        \n",
    "        # keep track of which files successfully open\n",
    "        try:\n",
    "            # open the file with xarray\n",
    "            ds = xr.open_dataset(path)\n",
    "    \n",
    "            # append file name to successful_files\n",
    "            successful_files[id_].append(file)\n",
    "    \n",
    "            # update maxes accordingly\n",
    "            # these *are* allowed to get mixed up between files right? in this case don't need to worry bout it\n",
    "            max_ncols[id_] = max(max_ncols[id_], ds.NCOLS)\n",
    "            max_nrows[id_] = max(max_nrows[id_], ds.NROWS)\n",
    "    \n",
    "            # these should not get mixed up between files right? or can they?\n",
    "            # if they do get mixed up, wouldn't it be a ill-defined grid?\n",
    "            # ref: https://stackoverflow.com/questions/18296755/python-max-function-using-key-and-lambda-expression\n",
    "            max_grid_x[id_][\"xorig\"] = max(max_grid_x[id_][\"xorig\"], ds.XORIG, key=abs)\n",
    "            max_grid_y[id_][\"yorig\"] = max(max_grid_y[id_][\"yorig\"], ds.YORIG, key=abs)\n",
    "            max_grid_x[id_][\"xcell\"] = max(max_grid_x[id_][\"xcell\"], ds.XCELL, key=abs)\n",
    "            max_grid_y[id_][\"ycell\"] = max(max_grid_y[id_][\"ycell\"], ds.YCELL, key=abs)\n",
    "    \n",
    "            # update sets\n",
    "            ncols[id_].add(ds.NCOLS)\n",
    "            nrows[id_].add(ds.NROWS)\n",
    "            xorigs[id_].add(ds.XORIG)\n",
    "            yorigs[id_].add(ds.YORIG)\n",
    "            xcells[id_].add(ds.XCELL)\n",
    "            ycells[id_].add(ds.YCELL)\n",
    "            \n",
    "        except:\n",
    "            # netcdf file does not exist\n",
    "            continue\n",
    "\n",
    "# Sort datasets' lists of successful files so they're in order of date\n",
    "for id_ in successful_files:\n",
    "    successful_files[id_] = np.sort(successful_files[id_]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fba2c72-e070-4b77-8060-f004bceb779b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: BSC18CA12-01\n",
      "Number of successful files: 1010\n",
      "Max cell sizes: max_ncols = 1081 and max_nrows = 381\n",
      "Max xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\n",
      "Max yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\n",
      "ncols: {1081, 1041}\n",
      "nrows: {381}\n",
      "xorigs: {-160.0, -156.0}\n",
      "yorigs: {32.0}\n",
      "xcells: {0.10000000149011612}\n",
      "ycells: {0.10000000149011612}\n",
      "\n",
      "dataset: BSC00CA12-01\n",
      "Number of successful files: 1200\n",
      "Max cell sizes: max_ncols = 1081 and max_nrows = 381\n",
      "Max xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\n",
      "Max yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\n",
      "ncols: {1081, 1041}\n",
      "nrows: {381}\n",
      "xorigs: {-160.0, -156.0}\n",
      "yorigs: {32.0}\n",
      "xcells: {0.10000000149011612}\n",
      "ycells: {0.10000000149011612}\n",
      "\n",
      "dataset: BSC06CA12-01\n",
      "Number of successful files: 997\n",
      "Max cell sizes: max_ncols = 1081 and max_nrows = 381\n",
      "Max xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\n",
      "Max yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\n",
      "ncols: {1081, 1041}\n",
      "nrows: {381}\n",
      "xorigs: {-160.0, -156.0}\n",
      "yorigs: {32.0}\n",
      "xcells: {0.10000000149011612}\n",
      "ycells: {0.10000000149011612}\n",
      "\n",
      "dataset: BSC12CA12-01\n",
      "Number of successful files: 1003\n",
      "Max cell sizes: max_ncols = 1081 and max_nrows = 381\n",
      "Max xorig & xcell: {'xorig': -160.0, 'xcell': 0.10000000149011612}\n",
      "Max yorig & ycell: {'yorig': 32.0, 'ycell': 0.10000000149011612}\n",
      "ncols: {1081, 1041}\n",
      "nrows: {381}\n",
      "xorigs: {-160.0, -156.0}\n",
      "yorigs: {32.0}\n",
      "xcells: {0.10000000149011612}\n",
      "ycells: {0.10000000149011612}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the information for all ids\n",
    "for id_ in ids:\n",
    "    print(f'dataset: {id_}')\n",
    "    print(f'Number of successful files: {len(successful_files[id_])}')\n",
    "    print(f'Max cell sizes: max_ncols = {max_ncols[id_]} and max_nrows = {max_nrows[id_]}')\n",
    "    print(f'Max xorig & xcell: {max_grid_x[id_]}')\n",
    "    print(f'Max yorig & ycell: {max_grid_y[id_]}')\n",
    "    print(f'ncols: {ncols[id_]}')\n",
    "    print(f'nrows: {nrows[id_]}')\n",
    "    print(f'xorigs: {xorigs[id_]}')\n",
    "    print(f'yorigs: {yorigs[id_]}')\n",
    "    print(f'xcells: {xcells[id_]}')\n",
    "    print(f'ycells: {ycells[id_]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f312f-f319-46a5-a7d2-1de4c49a9ed1",
   "metadata": {},
   "source": [
    "### Get latitude/longitude coordinates using the max values and non-max values AMONG ALL DATASETS, this is used for resampling during conversion\n",
    "\n",
    "Luckily, all datasets have the same 'smaller' and 'larger' lat/lon grid parameters :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b9fe211-ccb8-46df-93e9-92f7d18e702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parameters for bigger lat/lon\n",
    "max_xorig = max_grid_x[ids[0]]['xorig']\n",
    "max_xcell = max_grid_x[ids[0]]['xcell']\n",
    "max_yorig = max_grid_y[ids[0]]['yorig']\n",
    "max_ycell = max_grid_y[ids[0]]['ycell']\n",
    "\n",
    "# get arrays of bigger lat/lon grid\n",
    "big_lon = np.linspace(max_xorig, max_xorig + max_xcell * (max_ncols[ids[0]] - 1), max_ncols[ids[0]])\n",
    "big_lat = np.linspace(max_yorig, max_yorig + max_ycell * (max_nrows[ids[0]] - 1), max_nrows[ids[0]])\n",
    "\n",
    "# get coordinates made of new lat/lon arrays\n",
    "big_lon_pts, big_lat_pts = np.meshgrid(big_lon, big_lat)\n",
    "big_tups = np.array([tup for tup in zip(big_lon_pts.flatten(), big_lat_pts.flatten())])\n",
    "\n",
    "# get arrays of smaller lat/lon grid\n",
    "sml_ds = xr.open_dataset(firesmoke_dir + \"/BSC00CA12-01/dispersion_20210304.nc\")\n",
    "sml_lon = np.linspace(sml_ds.XORIG, sml_ds.XORIG + sml_ds.XCELL * (sml_ds.NCOLS - 1), sml_ds.NCOLS)\n",
    "sml_lat = np.linspace(sml_ds.YORIG, sml_ds.YORIG + sml_ds.YCELL * (sml_ds.NROWS - 1), sml_ds.NROWS)\n",
    "\n",
    "# get coordinates made of small lat/lon arrays\n",
    "sml_lon_pts, sml_lat_pts = np.meshgrid(sml_lon, sml_lat)\n",
    "sml_tups = np.array([tup for tup in zip(sml_lon_pts.flatten(), sml_lat_pts.flatten())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b68ee49-dabf-4258-95b9-707814280028",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## TESTING `resample_array` AND SCRIBBLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc424296-d880-462f-921b-d4966025e1d2",
   "metadata": {},
   "source": [
    "### This is plotting the oiginal 381x1041 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30986a19-aa82-4fd3-91fb-b76ed6811a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the PM25 values, squeeze out empty axis\n",
    "# vals = np.squeeze(sml_ds['PM25'].values)\n",
    "\n",
    "# # Perform the interpolation\n",
    "# arr = griddata(sml_tups, vals[15].flatten(), big_tups, method='cubic', fill_value=0)\n",
    "\n",
    "# # Any values that are less than a given threshold, make it 0\n",
    "# arr[arr < 1e-15] = 0\n",
    "\n",
    "# # Reshape the result to match the new grid shape\n",
    "# arr = arr.reshape((len(big_lat), len(big_lon)))\n",
    "\n",
    "# arr = arr.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f64beb-b62d-46b5-b7e2-1cf1d7796449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.min(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0975b-23c0-40f1-8666-ea9579b7d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(arr[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb1f2db-95a9-467b-af10-6f671f4f48d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's use matplotlib's imshow, since our data is on a grid\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # Initialize a figure and plot, so we can customize figure and plot of data\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/getting_started/index.html\n",
    "# my_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "\n",
    "# # Let's set some parameters to get the visualization we want\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # color PM25 values on a log scale, since values are small\n",
    "# my_norm = \"log\" \n",
    "# # this will number our x and y axes based on the longitude latitude range\n",
    "# my_extent = [np.min(sml_lon), np.max(sml_lon), np.min(sml_lat), np.max(sml_lat)]\n",
    "# # ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\n",
    "# my_aspect = 'auto'\n",
    "# # tell matplotlib, our origin is the lower-left corner\n",
    "# my_origin = 'lower'\n",
    "# # select a colormap for our plot and the color bar on the right\n",
    "# my_cmap = 'viridis'\n",
    "\n",
    "# # create our plot using imshow\n",
    "# plot = my_plt.imshow(arr, norm=my_norm, extent=my_extent, \n",
    "#           aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n",
    "\n",
    "# # draw coastlines\n",
    "# my_plt.coastlines()\n",
    "\n",
    "# # draw latitude longitude lines\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\n",
    "# my_plt.gridlines(draw_labels=True)\n",
    "\n",
    "# # add a colorbar to our figure, based on the plot we just made above\n",
    "# my_fig.colorbar(plot,location='right', label='ug/m^3')\n",
    "\n",
    "# # # Set x and y axis labels on our ax\n",
    "# # my_plt.set_xlabel('Longitude')\n",
    "# # my_plt.set_ylabel('Latitude')\n",
    "\n",
    "# # Set title of our figure\n",
    "# my_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n",
    "\n",
    "# # # Set title of our plot as the timestamp of our data\n",
    "# # my_plt.set_title(f'{my_timestamp}')\n",
    "\n",
    "# # Show the resulting visualization\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d18dc69-01cc-4fa9-91a5-1377bcdc6bea",
   "metadata": {},
   "source": [
    "### This is visualizing the resampled version of array above, from 381x1041 -> 381x1081 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59925a-f689-4c9f-a9a9-a897f45372e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's use matplotlib's imshow, since our data is on a grid\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # Initialize a figure and plot, so we can customize figure and plot of data\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/getting_started/index.html\n",
    "# my_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "\n",
    "# # Let's set some parameters to get the visualization we want\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # color PM25 values on a log scale, since values are small\n",
    "# my_norm = \"log\" \n",
    "# # this will number our x and y axes based on the longitude latitude range\n",
    "# my_extent = [np.min(big_lon), np.max(big_lon), np.min(big_lat), np.max(big_lat)]\n",
    "# # ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\n",
    "# my_aspect = 'auto'\n",
    "# # tell matplotlib, our origin is the lower-left corner\n",
    "# my_origin = 'lower'\n",
    "# # select a colormap for our plot and the color bar on the right\n",
    "# my_cmap = 'viridis'\n",
    "\n",
    "# # create our plot using imshow\n",
    "# plot = my_plt.imshow(arr_resamp, norm=my_norm, extent=my_extent, \n",
    "#           aspect=my_aspect, origin=my_origin, cmap=my_cmap, vmin=.00001, vmax=1)\n",
    "\n",
    "# # draw coastlines\n",
    "# my_plt.coastlines()\n",
    "\n",
    "# # draw latitude longitude lines\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\n",
    "# my_plt.gridlines(draw_labels=True)\n",
    "\n",
    "# # add a colorbar to our figure, based on the plot we just made above\n",
    "# my_fig.colorbar(plot,location='right', label='ug/m^3')\n",
    "\n",
    "# # # Set x and y axis labels on our ax\n",
    "# # my_plt.set_xlabel('Longitude')\n",
    "# # my_plt.set_ylabel('Latitude')\n",
    "\n",
    "# # Set title of our figure\n",
    "# my_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n",
    "\n",
    "# # # Set title of our plot as the timestamp of our data\n",
    "# # my_plt.set_title(f'{my_timestamp}')\n",
    "\n",
    "# # Show the resulting visualization\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f939c8-a3f9-4ec5-ab38-6c6cae22d486",
   "metadata": {},
   "source": [
    "## Determine sequence of files to load later for IDX conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb53b83-197f-4af8-ba35-118c50d5b699",
   "metadata": {},
   "source": [
    "### First determine what hours are available in all datasets, from there we construct final sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f7e05fb-8237-41cf-9b35-e7c25aed27c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for parsing time flags (TFLAG) from netcdf files\n",
    "def parse_tflag(tflag):\n",
    "    year = int(tflag[0] // 1000)\n",
    "    day_of_year = int(tflag[0] % 1000)\n",
    "    date = datetime.datetime(year, 1, 1) + datetime.timedelta(days=day_of_year - 1)\n",
    "\n",
    "    time_in_day = int(tflag[1])\n",
    "    hours = time_in_day // 10000\n",
    "    minutes = (time_in_day % 10000) // 100\n",
    "    seconds = time_in_day % 100\n",
    "\n",
    "    full_datetime = datetime.datetime(year, date.month, date.day, hours, minutes, seconds)\n",
    "    return full_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33f92213-d818-425b-a39c-e94a7b89e875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1010/1010 [00:16<00:00, 60.32it/s]\n",
      "100%|██████████| 1200/1200 [00:19<00:00, 61.64it/s]\n",
      "100%|██████████| 997/997 [00:16<00:00, 61.74it/s]\n",
      "100%|██████████| 1003/1003 [00:16<00:00, 61.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# get set of all available hours for each dataset using successful_files\n",
    "id_sets = {id_: {} for id_ in ids}\n",
    "\n",
    "for id_ in ids:    \n",
    "    # get successful files to add all successful hours to set\n",
    "    for file in tqdm(successful_files[id_]):\n",
    "        # get file's path\n",
    "        path = f'{firesmoke_dir}/{id_}/{file}'\n",
    "        \n",
    "        # open the file with xarray\n",
    "        ds = xr.open_dataset(path)\n",
    "\n",
    "        # add each available hour to successful_seq, store the index h, needed for idx conversion\n",
    "        for h in range(ds.sizes[\"TSTEP\"]):\n",
    "            id_sets[id_][(file, parse_tflag(ds['TFLAG'].values[h][0]))] = h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c91637a-b19b-44b0-8f4d-0023a0cb2334",
   "metadata": {},
   "source": [
    "### Ideally we use all dates, so step through all hours and grab from datasets accordingly.\n",
    "**Importantly, we should ideally use first six hours of each dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4a36e46-6316-4098-80c0-415acc945611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def next_id(curr_id):\n",
    "    '''\n",
    "    Return the string of the next dataset ID to use based on the current ID.\n",
    "    'Next' means, next most recently updated forecast after curr_id.\n",
    "    \n",
    "    Details on forecast update time can be found here: https://firesmoke.ca/forecasts/\n",
    "    \n",
    "    Listed in order: [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\n",
    "    \n",
    "    :param string curr_id: the ID used:\n",
    "    '''\n",
    "    ret = ''\n",
    "    \n",
    "    if curr_id == \"BSC18CA12-01\":\n",
    "        ret = \"BSC00CA12-01\"\n",
    "    if curr_id == \"BSC00CA12-01\": \n",
    "        ret = \"BSC06CA12-01\"\n",
    "    if curr_id == \"BSC06CA12-01\":\n",
    "        ret = \"BSC12CA12-01\"\n",
    "    if curr_id == \"BSC12CA12-01\":\n",
    "        ret = \"BSC18CA12-01\"\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def prev_id(curr_id):\n",
    "    '''\n",
    "    Return the string of the previous dataset ID to use based on the current ID.\n",
    "    'Previous' means, last most recently updated forecast before curr_id.\n",
    "    \n",
    "    Details on forecast update time can be found here: https://firesmoke.ca/forecasts/\n",
    "    \n",
    "    Listed in order: [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\n",
    "    \n",
    "    :param string curr_id: the ID used:\n",
    "    '''\n",
    "    ret = ''\n",
    "    \n",
    "    if curr_id == \"BSC18CA12-01\":\n",
    "        ret = \"BSC12CA12-01\"\n",
    "    if curr_id == \"BSC00CA12-01\": \n",
    "        ret = \"BSC18CA12-01\"\n",
    "    if curr_id == \"BSC06CA12-01\":\n",
    "        ret = \"BSC00CA12-01\"\n",
    "    if curr_id == \"BSC12CA12-01\":\n",
    "        ret = \"BSC06CA12-01\"\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "185ef9ad-0ee5-4119-acca-2897baeb008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_from_date(date, hour):\n",
    "    '''\n",
    "    Return the string of the dataset ID to use based on the date and hour given.\n",
    "    \n",
    "    We aim to use the dataset that provides the latest forecast update available for the hour.\n",
    "    \n",
    "    Details on forecast update time can be found here: https://firesmoke.ca/forecasts/\n",
    "    \n",
    "    :param datetime date: pandas timestamp of the YYYYMMDD date:\n",
    "    :param datetime hour: pandas timestamp of the 00:00:00 hour:\n",
    "    '''\n",
    "    ret = ''\n",
    "    \n",
    "    # based on the hour, grab from corresponding dataset id\n",
    "    if hour <= date.replace(hour=2):\n",
    "        # HERE WE NEED TO USE PRIOR DATE\n",
    "        ret = 'BSC12CA12-01'\n",
    "    if current_hour >= date.replace(hour=3) and date <= date.replace(hour=8):\n",
    "        ret = 'BSC18CA12-01'\n",
    "    if current_hour >= date.replace(hour=9) and date <= date.replace(hour=14):\n",
    "        ret = 'BSC00CA12-01'\n",
    "    if (current_hour >= date.replace(hour=15) and date <= date.replace(hour=20)):\n",
    "        ret = 'BSC06CA12-01'\n",
    "    if current_hour >= date.replace(hour=21):\n",
    "        ret = 'BSC18CA12-01'\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff967513-1ad4-4361-8000-89a834b85fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dispersion_date_str(date):\n",
    "    '''\n",
    "    For a given date object, generate the string for the dispersion file.\n",
    "    :param pd.Timestamp date: pandas timestamp of the date to make file name string out of\n",
    "    '''\n",
    "    return f'dispersion_{date.strftime(\"%Y%m%d\")}.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b967eca6-a9d8-457c-8de5-81e96a2f0582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(id_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9f3b1b3-1fbb-4f64-af65-63c759a9841a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_idx_calls(arr, curr_id, hour_file_tuple, id_sets):\n",
    "    '''\n",
    "    For the given array, append data specified by tuple if available in id_sets\n",
    "    :param list arr: array that holds final idx write sequence\n",
    "    :param tuple hour_file_tuple: tuple that holds the hour and file name to read\n",
    "    :param dict id_sets: dictionary that holds files that successfully open for each dataset:\n",
    "    '''\n",
    "    file_str = hour_file_tuple[0]\n",
    "    current_hour = hour_file_tuple[1]\n",
    "    \n",
    "    # get index of TFLAG of the hour in the file\n",
    "    tstep_idx = id_sets[curr_id][(file_str, current_hour)]\n",
    "    \n",
    "    # get file's path\n",
    "    path = f'{firesmoke_dir}/{curr_id}/{file_str}'\n",
    "    # open the file with xarray\n",
    "    ds = xr.open_dataset(path)\n",
    "    arr.append([curr_id, file_str, parse_tflag(ds['TFLAG'].values[tstep_idx][0]), tstep_idx])\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e724ad31-46c6-4353-9b80-31d822059806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dispersion file for current_date\n",
    "today_file_str = dispersion_date_str(current_date)\n",
    "\n",
    "# get dispersion file for previous 4 days, may be needed:\n",
    "last_4_days_strs = []\n",
    "for i in range(5):\n",
    "    d = current_date + datetime.timedelta(days=-i)\n",
    "    last_4_days_strs.append(dispersion_date_str(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75f4e1c4-6179-4ba8-b939-a8f180ffeced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Arrays to hold the final order we will index files\n",
    "idx_calls = []\n",
    "\n",
    "# Define the start and end dates we will step through\n",
    "start_date = datetime.datetime.strptime(\"20210304\", \"%Y%m%d\")\n",
    "end_date = datetime.datetime.strptime(\"20240627\", \"%Y%m%d\")\n",
    "\n",
    "# the dataset we wanna use\n",
    "curr_id_idx = 0\n",
    "\n",
    "# iterate over each day\n",
    "current_date = start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c61b4235-7d40-49f6-b6d2-aeaff5e0cdab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_hour' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current_date \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end_date:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# if we need to use yesterday dispersion file or not\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     yes_yesterday \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mcurrent_hour\u001b[49m \u001b[38;5;241m<\u001b[39m current_date \u001b[38;5;241m+\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtimedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# get dataset id\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         curr_id \u001b[38;5;241m=\u001b[39m get_id_from_date(current_date, current_hour)\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# BSC00CA12-01 generates first hours of the curr_date in yesterday's file\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# e.g. hours 12am-6am for January 2, 2023 are generated in dispersion_01012023.nc in BSC00CA12-01 dataset\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'current_hour' is not defined"
     ]
    }
   ],
   "source": [
    "# step through all files of dates specified\n",
    "while current_date <= end_date:\n",
    "    # iterate over each hour of the current day\n",
    "    current_hour = datetime.datetime(current_date.year, current_date.month, current_date.day)\n",
    "    # file to open\n",
    "    file_str = ''\n",
    "    \n",
    "    # tuple to hold the current hour and file to index\n",
    "    hour_file_tuple = (file_str, current_hour)\n",
    "    \n",
    "    # if we need to use yesterday dispersion file or not\n",
    "    yes_yesterday = False\n",
    "    \n",
    "    while current_hour < current_date + datetime.timedelta(days=1):\n",
    "        # get dataset id\n",
    "        curr_id = get_id_from_date(current_date, current_hour)\n",
    "\n",
    "        # BSC00CA12-01 generates first hours of the curr_date in yesterday's file\n",
    "        # e.g. hours 12am-6am for January 2, 2023 are generated in dispersion_01012023.nc in BSC00CA12-01 dataset\n",
    "        if curr_id == \"BSC00CA12-01\": # so use yesterday's file\n",
    "            file_str = dispersion_date_str(current_date + datetime.timedelta(days=-1))\n",
    "            yes_yesterday = True\n",
    "        else: # otherwise use today's file\n",
    "            file_str = dispersion_date_str(current_date)\n",
    "        \n",
    "        # if desired timestamp at desired dataset id is available, use it\n",
    "        if (file_str, current_hour) in id_sets[curr_id]:\n",
    "            idx_calls = update_idx_calls(idx_calls, curr_id, (file_str, current_hour), id_sets)\n",
    "        else: # search the others in order of most recent to least\n",
    "            found = 0\n",
    "            search_count = 0\n",
    "            \n",
    "            # if we are using yesterday file, we now use today file\n",
    "            if yes_yesterday:\n",
    "                file_str = dispersion_date_str(current_date)\n",
    "            \n",
    "            # first, do a pass over the other datasets\n",
    "            while found == 0 and search_count < 3:\n",
    "                # try previous id in sequence\n",
    "                curr_id = prev_id(curr_id)\n",
    "                # if timestamp is available, use it\n",
    "                if (file_str, current_hour) in id_sets[curr_id]:\n",
    "                    update_idx_calls(idx_calls, curr_id, (file_str, current_hour), id_sets)\n",
    "                    found = 1\n",
    "                else:\n",
    "                    search_count += 1\n",
    "\n",
    "            # if we *still* haven't found file, try a final check, check ALL datasets again but for yesterday str\n",
    "            if not found:\n",
    "                # use yesterday file\n",
    "                file_str = yest_file_str\n",
    "\n",
    "                # reset search counters and conditions\n",
    "                found = 0\n",
    "                search_count = 0\n",
    "\n",
    "                while found == 0 and search_count < 4:\n",
    "                    # try previous id in sequence\n",
    "                    curr_id_idx = (curr_id_idx - 1) % 4\n",
    "                    # if timestamp is available, use it\n",
    "                    if (file_str, current_hour) in id_sets[ids[curr_id_idx]]:\n",
    "                        # append dataset id, file name, and index of time step for idx conversion\n",
    "                        tstep_idx = id_sets[ids[curr_id_idx]][(file_str, current_hour)]\n",
    "                        \n",
    "                        # append dataset id, file name, and index of time step for idx conversion\n",
    "                        conversion_seq.append([ids[curr_id_idx], file_str, tstep_idx])\n",
    "    \n",
    "                        # get file's path\n",
    "                        path = f'{firesmoke_dir}/{ids[curr_id_idx]}/{file_str}'\n",
    "                        # open the file with xarray\n",
    "                        ds = xr.open_dataset(path)\n",
    "                        idx_calls.append([ids[curr_id_idx], file_str, parse_tflag(ds['TFLAG'].values[tstep_idx][0])])\n",
    "    \n",
    "                        found = 1\n",
    "                    else:  \n",
    "                        search_count += 1\n",
    "\n",
    "                    print(f'FAILED TO FIND, LAST RESORT:')\n",
    "                    print(f'found = {found}, search_count = {search_count}')\n",
    "                    print(f'ids[curr_id_idx] = {ids[curr_id_idx]}')\n",
    "                if not found:\n",
    "                    print('FAILed TO FIND ANYTHING')\n",
    "                    \n",
    "        # move to next hour\n",
    "        current_hour += datetime.timedelta(hours=1)\n",
    "\n",
    "    # move to the next day\n",
    "    current_date += datetime.timedelta(days=1)\n",
    "    print(f'{current_date}, ids[curr_id_idx] = {ids[curr_id_idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe5512b-2bae-4fbf-9792-f798ebefa2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "for c in idx_calls:\n",
    "    print(c)\n",
    "\n",
    "with open('idx_calls.txt', 'w') as f:\n",
    "    f.write(captured_output.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad0d93-02c2-4795-ae4a-0966225cfab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save conversion_seq to file\n",
    "with open('conversion_seq.pkl', 'wb') as f:\n",
    "    pickle.dump(conversion_seq, f)\n",
    "\n",
    "# save idx_calls to file\n",
    "with open('idx_calls.pkl', 'wb') as f:\n",
    "    pickle.dump(idx_calls, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54134708-4791-470d-87e8-772b2d403c16",
   "metadata": {},
   "source": [
    "### Let's check what hours are missing and _why_ **before** doing conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28a6cb5a-acc6-4b6d-95b8-40c426a55650",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load conversion_seq from a file\n",
    "with open('conversion_seq.pkl', 'rb') as f:\n",
    "    conversion_seq = pickle.load(f)\n",
    "\n",
    "# Load idx_calls from a file\n",
    "with open('idx_calls.pkl', 'rb') as f:\n",
    "    idx_calls = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910833d-b97b-40f2-ae09-e792fb724333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# last 3 hours of dataset\n",
    "idx_calls[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b95b67-a9e1-499b-bc23-d6313fff9254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first 3 hours of dataset\n",
    "idx_calls[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb5262-6c37-4014-8e19-9c72817e789b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the start and end dates\n",
    "start_date = pd.Timestamp(datetime.datetime.strptime(\"20210303\", \"%Y%m%d\"))\n",
    "end_date = pd.Timestamp(datetime.datetime.strptime(\"20240627\", \"%Y%m%d\"))\n",
    "\n",
    "# Get all hours between the start and end dates\n",
    "desired_tflag_set = {start_date + pd.Timedelta(hours=x) for x in range(int((end_date - start_date).total_seconds() // 3600) + 1)}\n",
    "\n",
    "print(f'There are {len(desired_tflag_set)} hours between 3/3/21 and 6/27/24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7026a67-ea6a-4150-a74e-9afc3a3de153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all hours in idx conversion\n",
    "idx_hours = {call[2] for call in idx_calls}\n",
    "\n",
    "# Get set of missing hours\n",
    "hours_missing_set = desired_tflag_set.difference(idx_hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc4a99-7495-4cdc-bb96-ae48a551caba",
   "metadata": {},
   "source": [
    "### For all missing hours, try opening files to find hour and report errors seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62125c6-c3de-4d87-87a3-c7a00af74aa4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dictionary to hold where one can find hour in files\n",
    "hours_found = {h : [] for h in hours_missing_set}\n",
    "\n",
    "for h in hours_missing_set:\n",
    "    # open the dispersion.nc file on the date of the hour and 3 days before\n",
    "    for i in range(5):\n",
    "        # get date string and build file name\n",
    "        curr_date = h + pd.Timedelta(days=-i)\n",
    "        file_name = f'dispersion_{curr_date.strftime(\"%Y%m%d\")}.nc'\n",
    "        \n",
    "        # search for the file in each dataset\n",
    "        for id_ in ids:\n",
    "            # get file's path\n",
    "            path = f'{firesmoke_dir}/{id_}/{file_name}'\n",
    "            \n",
    "            # try opening the file with xarray\n",
    "            try:\n",
    "                ds = xr.open_dataset(path)\n",
    "\n",
    "                # get timestamps\n",
    "                tflags = [pd.Timestamp(parse_tflag(t[0])) for t in ds['TFLAG'].values]\n",
    "                \n",
    "                # if h is in timestamps, add the file to hours_found dictionary\n",
    "                hours_found[h].append(path) \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'failed to open {path}, exception: {e}')\n",
    "                print('---')\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf195292-267b-4168-9704-b809bfb1dcf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for h in hours_found.keys():\n",
    "    # print if one of the hours' lists have no files..\n",
    "    if len(hours_found[h]) == 0:\n",
    "        print(f'hour {h} was in no files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3faab6-29d0-425e-a8fb-c4801fb9d7b6",
   "metadata": {},
   "source": [
    "At this point the issue is not a lack of missing netCDF files, it is of incorrect sequencing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd9ca5-ca24-4f90-afc5-b61d0c3b97e3",
   "metadata": {},
   "source": [
    "## Do conversion from netCDF files to IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e390a-bff6-46a3-8c5a-7139b56536c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#********** FOR TESTING\n",
    "# my_seq = conversion_seq[22200:22200+145]\n",
    "my_seq = conversion_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8bf6a5-0d2c-4b39-a2be-fbfb80a13dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'len(conversion_seq) = {len(conversion_seq)}')\n",
    "print(f'len(idx_calls) = {len(idx_calls)}')\n",
    "print(f'len(my_seq) = {len(my_seq)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e63f9-b881-488b-a7cb-bfa7ccf9872e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Create idx file of i'th dataset\n",
    "# # useful for dealing with fields that are not all the same size:\n",
    "# # https://github.com/sci-visus/OpenVisus/blob/master/Samples/jupyter/nasa_conversion_example.ipynb\n",
    "   \n",
    "# # create OpenVisus field for the pm25 variable\n",
    "# f = Field('PM25', 'float32')\n",
    "\n",
    "# # create the idx file for this dataset using field f\n",
    "# # dims is maximum array size, we will resample data accordingly to fit this\n",
    "# # time is number of files * 24 (hours)\n",
    "# db = CreateIdx(url=idx_dir + '/firesmoke.idx', fields=[f], \n",
    "#                dims=[int(max_ncols[ids[0]]), int(max_nrows[ids[0]])], time=[0, len(my_seq) - 1, '%00000000d/'])\n",
    "\n",
    "# # to track what timestep we are on in idx\n",
    "# tstep = 0\n",
    "\n",
    "# # threshold to use to change small-enough resampled values to 0\n",
    "# thresh = 1e-15\n",
    "\n",
    "# for call in tqdm(my_seq):\n",
    "#     # get instructions from call\n",
    "#     curr_id = call[0]\n",
    "#     curr_file = call[1]\n",
    "#     tstep_index = call[2]\n",
    "#     # open the file with xarray\n",
    "#     ds = xr.open_dataset(f'{firesmoke_dir}/{curr_id}/{curr_file}')\n",
    "    \n",
    "#     # Get the PM25 values, squeeze out empty axis\n",
    "#     file_vals = np.squeeze(ds['PM25'].values)\n",
    "    \n",
    "#     # to decide if we need to resample or not\n",
    "#     resamp = ds.XORIG != max_xorig\n",
    "    \n",
    "#     # resample data if not already on max lat/lon grid\n",
    "#     if resamp:\n",
    "#         # Perform the interpolation\n",
    "#         file_vals_resamp = griddata(sml_tups, file_vals[tstep_index].flatten(), big_tups, method='cubic', fill_value=0)\n",
    "        \n",
    "#         # Any values that are less than a given threshold, make it 0\n",
    "#         file_vals_resamp[file_vals_resamp < thresh] = 0\n",
    "        \n",
    "#         # Reshape the result to match the new grid shape\n",
    "#         file_vals_resamp = file_vals_resamp.reshape((len(big_lat), len(big_lon)))\n",
    "#         # Write resampled values at hour h to timestep t and field f\n",
    "#         db.write(data=file_vals_resamp.astype(np.float32),field=f,time=tstep)\n",
    "#     else:\n",
    "#         # Write original values at hour h to timestep t and field f\n",
    "#         db.write(data=file_vals[tstep_index], field=f, time=tstep)\n",
    "\n",
    "#     # move to next timestep in IDX\n",
    "#     tstep = tstep + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b908337-8454-4f24-9cec-4325ad74f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # go to idx data directory\n",
    "# os.chdir(idx_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3af4da-d469-44b9-9c90-20216304c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compress dataset\n",
    "# db.compressDataset(['zip'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
