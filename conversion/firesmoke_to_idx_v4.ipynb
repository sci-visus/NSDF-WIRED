{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca8ac1b-4eaa-478a-ad7e-1364e6c1791e",
   "metadata": {},
   "source": [
    "# Firesmoke Data Conversion to IDX using OpenVisus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f4506-47b9-452e-908e-15e532e1b801",
   "metadata": {},
   "source": [
    "## Import necessary libraries, install them if you do not have them. This was developed in Python 3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f499004e-bebd-4abf-8885-f16ce6ad9531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used to read/manipulate netCDF data\n",
    "import xarray as xr\n",
    "\n",
    "# Used to convert to .idx\n",
    "from OpenVisus import *\n",
    "\n",
    "# Used for numerical work\n",
    "import numpy as np\n",
    "\n",
    "# Used for processing netCDF time data\n",
    "import datetime\n",
    "\n",
    "# Used for interacting with OS file system (to get directory file names)\n",
    "import os\n",
    "\n",
    "# To load/save final sequence array to file\n",
    "import pickle\n",
    "\n",
    "# Used for resampling arrays to fit the same lat/lon grid\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# for checking and using timestamps\n",
    "import pandas as pd\n",
    "\n",
    "# Accessory, used to generate progress bar for running for loops\n",
    "# from tqdm.notebook import tqdm\n",
    "# import ipywidgets\n",
    "# import jupyterlab_widgets\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3169e-d1dd-4050-811a-6fd661442c81",
   "metadata": {},
   "source": [
    "## Get relevant directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b0c7861-34fb-464b-95b1-4a1aa4a8ba21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ******* THIS IS WHEN RUNNING FROM ATLANTIS.SCI **************\n",
    "firesmoke_dir = \"/usr/sci/cedmav/data/firesmoke\"\n",
    "\n",
    "# path to save idx file and data\n",
    "idx_dir = \"/usr/sci/scratch_nvme/arleth/idx/firesmoke\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d88f90-0370-481c-93be-e3c1e1a10ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get metadata of datasets, had to be obtained manually\n",
    "ids = [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\n",
    "start_dates = [\"20210304\", \"20210304\", \"20210304\", \"20210303\"]\n",
    "end_dates = [\"20240627\", \"20240627\", \"20240627\", \"20240627\"]\n",
    "\n",
    "id_dates = {ids[i]: {\"start_date\": start_dates[i], \"end_date\": end_dates[i]} for i in range(len(ids))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32528eee-cd60-44f6-9e14-cae79c5ce478",
   "metadata": {},
   "source": [
    "## Gather information about the metadata of our files, since it is inconsistent file to file. We need to know what to normalize across all files.\n",
    "\n",
    "### In particular:\n",
    "#### 1. Count number of files there are per firesmoke directory.\n",
    "#### 2. Determine maximum row,col dimension sizes for pm25 array.\n",
    "#### 3. Determine maximum latitude longitude grid parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c92053f-9e91-4e6d-b46b-a9efa04c6d58",
<<<<<<< Updated upstream
   "metadata": {
    "tags": []
   },
   "outputs": [],
=======
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/usr/sci/cedmav/data/firesmoke/BSC18CA12-01/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m ycells \u001b[38;5;241m=\u001b[39m {id_: \u001b[38;5;28mset\u001b[39m() \u001b[38;5;28;01mfor\u001b[39;00m id_ \u001b[38;5;129;01min\u001b[39;00m ids}\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m id_ \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# get list of netcdf file names for each dataset\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     file_names \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfiresmoke_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mid_\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# try opening each file, process only if it successfully opens\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m tqdm(file_names):\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m# get file's path\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/sci/cedmav/data/firesmoke/BSC18CA12-01/'"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "# List of all files that are available from UBC\n",
    "successful_files = {id_: [] for id_ in ids}\n",
    "\n",
    "# Variables to hold maxes, also to track the unique max values\n",
    "max_ncols = {id_: 0 for id_ in ids}\n",
    "max_nrows = {id_: 0 for id_ in ids}\n",
    "ncols = {id_: set() for id_ in ids}\n",
    "nrows = {id_: set() for id_ in ids}\n",
    "\n",
    "# Max grid dimensions\n",
    "max_grid_x = {id_: {\"xorig\": 0.0, \"xcell\": 0.0} for id_ in ids}\n",
    "max_grid_y = {id_: {\"yorig\": 0.0, \"ycell\": 0.0} for id_ in ids}\n",
    "xorigs = {id_: set() for id_ in ids}\n",
    "xcells = {id_: set() for id_ in ids}\n",
    "yorigs = {id_: set() for id_ in ids}\n",
    "ycells = {id_: set() for id_ in ids}\n",
    "\n",
    "for id_ in ids:\n",
    "    # get list of netcdf file names for each dataset\n",
    "    file_names = os.listdir(f'{firesmoke_dir}/{id_}/')\n",
    "    \n",
    "    # try opening each file, process only if it successfully opens\n",
    "    for file in tqdm(file_names):\n",
    "        # get file's path\n",
    "        path = f'{firesmoke_dir}/{id_}/{file}'\n",
    "        \n",
    "        # keep track of which files successfully open\n",
    "        try:\n",
    "            # open the file with xarray\n",
    "            ds = xr.open_dataset(path)\n",
    "    \n",
    "            # append file name to successful_files\n",
    "            successful_files[id_].append(file)\n",
    "    \n",
    "            # update maxes accordingly\n",
    "            # these *are* allowed to get mixed up between files right? in this case don't need to worry bout it\n",
    "            max_ncols[id_] = max(max_ncols[id_], ds.NCOLS)\n",
    "            max_nrows[id_] = max(max_nrows[id_], ds.NROWS)\n",
    "    \n",
    "            # these should not get mixed up between files right? or can they?\n",
    "            # if they do get mixed up, wouldn't it be a ill-defined grid?\n",
    "            # ref: https://stackoverflow.com/questions/18296755/python-max-function-using-key-and-lambda-expression\n",
    "            max_grid_x[id_][\"xorig\"] = max(max_grid_x[id_][\"xorig\"], ds.XORIG, key=abs)\n",
    "            max_grid_y[id_][\"yorig\"] = max(max_grid_y[id_][\"yorig\"], ds.YORIG, key=abs)\n",
    "            max_grid_x[id_][\"xcell\"] = max(max_grid_x[id_][\"xcell\"], ds.XCELL, key=abs)\n",
    "            max_grid_y[id_][\"ycell\"] = max(max_grid_y[id_][\"ycell\"], ds.YCELL, key=abs)\n",
    "    \n",
    "            # update sets\n",
    "            ncols[id_].add(ds.NCOLS)\n",
    "            nrows[id_].add(ds.NROWS)\n",
    "            xorigs[id_].add(ds.XORIG)\n",
    "            yorigs[id_].add(ds.YORIG)\n",
    "            xcells[id_].add(ds.XCELL)\n",
    "            ycells[id_].add(ds.YCELL)\n",
    "            \n",
    "        except:\n",
    "            # netcdf file does not exist\n",
    "            continue\n",
    "\n",
    "# Sort datasets' lists of successful files so they're in order of date\n",
    "for id_ in successful_files:\n",
    "    successful_files[id_] = np.sort(successful_files[id_]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba2c72-e070-4b77-8060-f004bceb779b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the information for all ids\n",
    "for id_ in ids:\n",
    "    print(f'dataset: {id_}')\n",
    "    print(f'Number of successful files: {len(successful_files[id_])}')\n",
    "    print(f'Max cell sizes: max_ncols = {max_ncols[id_]} and max_nrows = {max_nrows[id_]}')\n",
    "    print(f'Max xorig & xcell: {max_grid_x[id_]}')\n",
    "    print(f'Max yorig & ycell: {max_grid_y[id_]}')\n",
    "    print(f'ncols: {ncols[id_]}')\n",
    "    print(f'nrows: {nrows[id_]}')\n",
    "    print(f'xorigs: {xorigs[id_]}')\n",
    "    print(f'yorigs: {yorigs[id_]}')\n",
    "    print(f'xcells: {xcells[id_]}')\n",
    "    print(f'ycells: {ycells[id_]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f312f-f319-46a5-a7d2-1de4c49a9ed1",
   "metadata": {},
   "source": [
    "### Get latitude/longitude coordinates using the max values and non-max values AMONG ALL DATASETS, this is used for resampling during conversion\n",
    "\n",
    "Luckily, all datasets have the same 'smaller' and 'larger' lat/lon grid parameters :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9fe211-ccb8-46df-93e9-92f7d18e702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parameters for bigger lat/lon\n",
    "max_xorig = max_grid_x[ids[0]]['xorig']\n",
    "max_xcell = max_grid_x[ids[0]]['xcell']\n",
    "max_yorig = max_grid_y[ids[0]]['yorig']\n",
    "max_ycell = max_grid_y[ids[0]]['ycell']\n",
    "\n",
    "# get arrays of bigger lat/lon grid\n",
    "big_lon = np.linspace(max_xorig, max_xorig + max_xcell * (max_ncols[ids[0]] - 1), max_ncols[ids[0]])\n",
    "big_lat = np.linspace(max_yorig, max_yorig + max_ycell * (max_nrows[ids[0]] - 1), max_nrows[ids[0]])\n",
    "\n",
    "# get coordinates made of new lat/lon arrays\n",
    "big_lon_pts, big_lat_pts = np.meshgrid(big_lon, big_lat)\n",
    "big_tups = np.array([tup for tup in zip(big_lon_pts.flatten(), big_lat_pts.flatten())])\n",
    "\n",
    "# get arrays of smaller lat/lon grid\n",
    "sml_ds = xr.open_dataset(firesmoke_dir + \"/BSC00CA12-01/dispersion_20210304.nc\")\n",
    "sml_lon = np.linspace(sml_ds.XORIG, sml_ds.XORIG + sml_ds.XCELL * (sml_ds.NCOLS - 1), sml_ds.NCOLS)\n",
    "sml_lat = np.linspace(sml_ds.YORIG, sml_ds.YORIG + sml_ds.YCELL * (sml_ds.NROWS - 1), sml_ds.NROWS)\n",
    "\n",
    "# get coordinates made of small lat/lon arrays\n",
    "sml_lon_pts, sml_lat_pts = np.meshgrid(sml_lon, sml_lat)\n",
    "sml_tups = np.array([tup for tup in zip(sml_lon_pts.flatten(), sml_lat_pts.flatten())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b68ee49-dabf-4258-95b9-707814280028",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TESTING `resample_array` AND SCRIBBLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc424296-d880-462f-921b-d4966025e1d2",
   "metadata": {},
   "source": [
    "### This is plotting the oiginal 381x1041 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30986a19-aa82-4fd3-91fb-b76ed6811a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the PM25 values, squeeze out empty axis\n",
    "# vals = np.squeeze(sml_ds['PM25'].values)\n",
    "\n",
    "# # Perform the interpolation\n",
    "# arr = griddata(sml_tups, vals[15].flatten(), big_tups, method='cubic', fill_value=0)\n",
    "\n",
    "# # Any values that are less than a given threshold, make it 0\n",
    "# arr[arr < 1e-15] = 0\n",
    "\n",
    "# # Reshape the result to match the new grid shape\n",
    "# arr = arr.reshape((len(big_lat), len(big_lon)))\n",
    "\n",
    "# arr = arr.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f64beb-b62d-46b5-b7e2-1cf1d7796449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.min(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0975b-23c0-40f1-8666-ea9579b7d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(arr[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb1f2db-95a9-467b-af10-6f671f4f48d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's use matplotlib's imshow, since our data is on a grid\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # Initialize a figure and plot, so we can customize figure and plot of data\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/getting_started/index.html\n",
    "# my_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "\n",
    "# # Let's set some parameters to get the visualization we want\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # color PM25 values on a log scale, since values are small\n",
    "# my_norm = \"log\" \n",
    "# # this will number our x and y axes based on the longitude latitude range\n",
    "# my_extent = [np.min(sml_lon), np.max(sml_lon), np.min(sml_lat), np.max(sml_lat)]\n",
    "# # ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\n",
    "# my_aspect = 'auto'\n",
    "# # tell matplotlib, our origin is the lower-left corner\n",
    "# my_origin = 'lower'\n",
    "# # select a colormap for our plot and the color bar on the right\n",
    "# my_cmap = 'viridis'\n",
    "\n",
    "# # create our plot using imshow\n",
    "# plot = my_plt.imshow(arr, norm=my_norm, extent=my_extent, \n",
    "#           aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n",
    "\n",
    "# # draw coastlines\n",
    "# my_plt.coastlines()\n",
    "\n",
    "# # draw latitude longitude lines\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\n",
    "# my_plt.gridlines(draw_labels=True)\n",
    "\n",
    "# # add a colorbar to our figure, based on the plot we just made above\n",
    "# my_fig.colorbar(plot,location='right', label='ug/m^3')\n",
    "\n",
    "# # # Set x and y axis labels on our ax\n",
    "# # my_plt.set_xlabel('Longitude')\n",
    "# # my_plt.set_ylabel('Latitude')\n",
    "\n",
    "# # Set title of our figure\n",
    "# my_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n",
    "\n",
    "# # # Set title of our plot as the timestamp of our data\n",
    "# # my_plt.set_title(f'{my_timestamp}')\n",
    "\n",
    "# # Show the resulting visualization\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d18dc69-01cc-4fa9-91a5-1377bcdc6bea",
   "metadata": {},
   "source": [
    "### This is visualizing the resampled version of array above, from 381x1041 -> 381x1081 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59925a-f689-4c9f-a9a9-a897f45372e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's use matplotlib's imshow, since our data is on a grid\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # Initialize a figure and plot, so we can customize figure and plot of data\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/getting_started/index.html\n",
    "# my_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "\n",
    "# # Let's set some parameters to get the visualization we want\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # color PM25 values on a log scale, since values are small\n",
    "# my_norm = \"log\" \n",
    "# # this will number our x and y axes based on the longitude latitude range\n",
    "# my_extent = [np.min(big_lon), np.max(big_lon), np.min(big_lat), np.max(big_lat)]\n",
    "# # ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\n",
    "# my_aspect = 'auto'\n",
    "# # tell matplotlib, our origin is the lower-left corner\n",
    "# my_origin = 'lower'\n",
    "# # select a colormap for our plot and the color bar on the right\n",
    "# my_cmap = 'viridis'\n",
    "\n",
    "# # create our plot using imshow\n",
    "# plot = my_plt.imshow(arr_resamp, norm=my_norm, extent=my_extent, \n",
    "#           aspect=my_aspect, origin=my_origin, cmap=my_cmap, vmin=.00001, vmax=1)\n",
    "\n",
    "# # draw coastlines\n",
    "# my_plt.coastlines()\n",
    "\n",
    "# # draw latitude longitude lines\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\n",
    "# my_plt.gridlines(draw_labels=True)\n",
    "\n",
    "# # add a colorbar to our figure, based on the plot we just made above\n",
    "# my_fig.colorbar(plot,location='right', label='ug/m^3')\n",
    "\n",
    "# # # Set x and y axis labels on our ax\n",
    "# # my_plt.set_xlabel('Longitude')\n",
    "# # my_plt.set_ylabel('Latitude')\n",
    "\n",
    "# # Set title of our figure\n",
    "# my_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n",
    "\n",
    "# # # Set title of our plot as the timestamp of our data\n",
    "# # my_plt.set_title(f'{my_timestamp}')\n",
    "\n",
    "# # Show the resulting visualization\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f939c8-a3f9-4ec5-ab38-6c6cae22d486",
   "metadata": {},
   "source": [
    "## Determine sequence of files to load later for IDX conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb53b83-197f-4af8-ba35-118c50d5b699",
   "metadata": {},
   "source": [
    "### First determine what hours are available in all datasets, from there we construct final sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e05fb-8237-41cf-9b35-e7c25aed27c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for parsing time flags (TFLAG) from netcdf files\n",
    "def parse_tflag(tflag):\n",
    "    year = int(tflag[0] // 1000)\n",
    "    day_of_year = int(tflag[0] % 1000)\n",
    "    date = datetime.datetime(year, 1, 1) + datetime.timedelta(days=day_of_year - 1)\n",
    "\n",
    "    time_in_day = int(tflag[1])\n",
    "    hours = time_in_day // 10000\n",
    "    minutes = (time_in_day % 10000) // 100\n",
    "    seconds = time_in_day % 100\n",
    "\n",
    "    full_datetime = datetime.datetime(year, date.month, date.day, hours, minutes, seconds)\n",
    "    return full_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f92213-d818-425b-a39c-e94a7b89e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get set of all available hours for each dataset using successful_files\n",
    "id_sets = {id_: {} for id_ in ids}\n",
    "\n",
    "for id_ in ids:    \n",
    "    # get successful files to add all successful hours to set\n",
    "    for file in tqdm(successful_files[id_]):\n",
    "        # get file's path\n",
    "        path = f'{firesmoke_dir}/{id_}/{file}'\n",
    "        \n",
    "        # open the file with xarray\n",
    "        ds = xr.open_dataset(path)\n",
    "\n",
    "        # add each available hour to successful_seq, store the index h, needed for idx conversion\n",
    "        for h in range(ds.sizes[\"TSTEP\"]):\n",
    "            id_sets[id_][(file, parse_tflag(ds['TFLAG'].values[h][0]))] = h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c91637a-b19b-44b0-8f4d-0023a0cb2334",
   "metadata": {},
   "source": [
    "### Ideally we use all dates, so step through all hours and grab from datasets accordingly.\n",
    "**Importantly, we should ideally use first six hours of each dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a36e46-6316-4098-80c0-415acc945611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def next_id(curr_id, verbose):\n",
    "    '''\n",
    "    Return the string of the next dataset ID to use based on the current ID.\n",
    "    'Next' means, next most recently updated forecast after curr_id.\n",
    "    \n",
    "    Details on forecast update time can be found here: https://firesmoke.ca/forecasts/\n",
    "    \n",
    "    Listed in order: [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\n",
    "    \n",
    "    :param string curr_id: the ID used:\n",
    "    '''\n",
    "    ret = ''\n",
    "    \n",
    "    if curr_id == \"BSC18CA12-01\":\n",
    "        ret = \"BSC00CA12-01\"\n",
    "    if curr_id == \"BSC00CA12-01\": \n",
    "        ret = \"BSC06CA12-01\"\n",
    "    if curr_id == \"BSC06CA12-01\":\n",
    "        ret = \"BSC12CA12-01\"\n",
    "    if curr_id == \"BSC12CA12-01\":\n",
    "        ret = \"BSC18CA12-01\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'next_id({curr_id}) = {ret}')\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def prev_id(curr_id, verbose):\n",
    "    '''\n",
    "    Return the string of the previous dataset ID to use based on the current ID.\n",
    "    'Previous' means, last most recently updated forecast before curr_id.\n",
    "    \n",
    "    Details on forecast update time can be found here: https://firesmoke.ca/forecasts/\n",
    "    \n",
    "    Listed in order: [\"BSC18CA12-01\", \"BSC00CA12-01\", \"BSC06CA12-01\", \"BSC12CA12-01\"]\n",
    "    \n",
    "    :param string curr_id: the ID used:\n",
    "    '''\n",
    "    ret = ''\n",
    "    \n",
    "    if curr_id == \"BSC18CA12-01\":\n",
    "        ret = \"BSC12CA12-01\"\n",
    "    if curr_id == \"BSC00CA12-01\": \n",
    "        ret = \"BSC18CA12-01\"\n",
    "    if curr_id == \"BSC06CA12-01\":\n",
    "        ret = \"BSC00CA12-01\"\n",
    "    if curr_id == \"BSC12CA12-01\":\n",
    "        ret = \"BSC06CA12-01\"\n",
    "        \n",
    "    if verbose:\n",
    "        print(f'prev_id({curr_id}) = {ret}')\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ef9ad-0ee5-4119-acca-2897baeb008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_from_date(date, verbose):\n",
    "    '''\n",
    "    Return the string of the dataset ID to use based on the date and hour given.\n",
    "    \n",
    "    We aim to use the dataset that provides the latest forecast update available for the hour.\n",
    "    \n",
    "    Details on forecast update time can be found here: https://firesmoke.ca/forecasts/\n",
    "    \n",
    "    :param datetime date: pandas timestamp of the YYYYMMDD 00:00:00 date:\n",
    "    '''\n",
    "    ret = ''\n",
    "    \n",
    "    # based on the hour, grab from corresponding dataset id\n",
    "    if date <= date.replace(hour=2):\n",
    "        # HERE WE NEED TO USE PRIOR DATE\n",
    "        ret = 'BSC12CA12-01'\n",
    "    if date >= date.replace(hour=3) and date <= date.replace(hour=8):\n",
    "        ret = 'BSC18CA12-01'\n",
    "    if date >= date.replace(hour=9) and date <= date.replace(hour=14):\n",
    "        ret = 'BSC00CA12-01'\n",
    "    if (date >= date.replace(hour=15) and date <= date.replace(hour=20)):\n",
    "        ret = 'BSC06CA12-01'\n",
    "    if date >= date.replace(hour=21):\n",
    "        ret = 'BSC12CA12-01'\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'get_id_from_date({date}) = {ret}')\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff967513-1ad4-4361-8000-89a834b85fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dispersion_date_str(date, id_, verbose):\n",
    "    '''\n",
    "    For a given date object and dataset id, generate the string for the dispersion file.\n",
    "    :param pd.Timestamp date: pandas timestamp of the date to make file name string out of:\n",
    "    :param string id_: string with the dataset id to use\n",
    "    '''\n",
    "    ret = ''\n",
    "\n",
    "   # BSC00CA12-01 generates first hours of the given date in yesterday's file\n",
    "    # e.g. hours 12am-6am for January 2, 2023 are generated in dispersion_01012023.nc in BSC00CA12-01 dataset\n",
    "    if id_ == 'BSC12CA12-01':\n",
    "        new_date = date + datetime.timedelta(days=-1)\n",
    "        ret = f'dispersion_{new_date.strftime(\"%Y%m%d\")}.nc'\n",
    "    else:\n",
    "        ret = f'dispersion_{date.strftime(\"%Y%m%d\")}.nc'\n",
    "\n",
    "    if verbose:\n",
    "        print(f'dispersion_date_str({date}, {id_}) = {ret}')\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f3b1b3-1fbb-4f64-af65-63c759a9841a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_idx_calls(arr, curr_id, hour_file_tuple, id_sets):\n",
    "    '''\n",
    "    For the given array, append data specified by tuple if available in id_sets\n",
    "    :param list arr: array that holds final idx write sequence\n",
    "    :param tuple hour_file_tuple: tuple that holds the hour and file name to read\n",
    "    :param dict id_sets: dictionary that holds files that successfully open for each dataset:\n",
    "    '''\n",
    "    file_str = hour_file_tuple[0]\n",
    "    current_hour = hour_file_tuple[1]\n",
    "    \n",
    "    # get index of TFLAG of the hour in the file\n",
    "    tstep_idx = id_sets[curr_id][(file_str, current_hour)]\n",
    "    \n",
    "    # get file's path\n",
    "    path = f'{firesmoke_dir}/{curr_id}/{file_str}'\n",
    "    # open the file with xarray\n",
    "    ds = xr.open_dataset(path)\n",
    "    arr.append([curr_id, file_str, parse_tflag(ds['TFLAG'].values[tstep_idx][0]), tstep_idx])\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f4e1c4-6179-4ba8-b939-a8f180ffeced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Arrays to hold the final order we will index files\n",
    "idx_calls = []\n",
    "\n",
    "# Define the start and end dates we will step through\n",
    "start_date = datetime.datetime.strptime(\"20210304\", \"%Y%m%d\")\n",
    "end_date = datetime.datetime.strptime(\"20240627\", \"%Y%m%d\")\n",
    "\n",
    "# iterate over each day\n",
    "current_date = start_date\n",
    "\n",
    "# iterate over each hour of the current day\n",
    "current_hour = datetime.datetime(current_date.year, current_date.month, current_date.day)\n",
    "\n",
    "# file to open\n",
    "file_str = ''\n",
    "\n",
    "# if we need to use yesterday dispersion file or not\n",
    "yes_yesterday = False\n",
    "\n",
    "# tell functions to print for debugging\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61b4235-7d40-49f6-b6d2-aeaff5e0cdab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "while current_date <= end_date:    \n",
    "    while current_hour < current_date + datetime.timedelta(days=1):        \n",
    "        # set search counters and conditions\n",
    "        prev_day_count = 0\n",
    "        found = 0\n",
    "\n",
    "        # search for best dataset id from current date and previous 4 days\n",
    "        while found == 0 and prev_day_count <= 4:\n",
    "            # to hold which current date we're trying\n",
    "            curr_date = current_hour + datetime.timedelta(days=-prev_day_count)\n",
    "\n",
    "            # get dataset id that should contain newest forecast for current hour\n",
    "            curr_id = get_id_from_date(curr_date, verbose)\n",
    "\n",
    "            # initialize dataset search count\n",
    "            search_count = 0\n",
    "\n",
    "            while found == 0 and search_count < 4:\n",
    "                # get dispersion file to load\n",
    "                file_str = dispersion_date_str(curr_date, curr_id, verbose)\n",
    "\n",
    "                # if timestamp is available, use it\n",
    "                if (file_str, current_hour) in id_sets[curr_id]:\n",
    "                    update_idx_calls(idx_calls, curr_id, (file_str, current_hour), id_sets)\n",
    "                    print(f'USING {curr_id}: {(file_str, current_hour)}')\n",
    "                    found = 1\n",
    "                else:\n",
    "                    print(f'{(file_str, current_hour)} NOT IN id_sets[{curr_id}]')\n",
    "                    search_count += 1\n",
    "\n",
    "                # get next previous dataset id that should contain newest forecast for current hour\n",
    "                curr_id = prev_id(curr_id, verbose)\n",
    "            # try the next previous day\n",
    "            prev_day_count += 1\n",
    "            print('---')\n",
    "\n",
    "        # move to next hour\n",
    "        current_hour += datetime.timedelta(hours=1)\n",
    "\n",
    "        print('~~~~~')\n",
    "\n",
    "    # move to the next day\n",
    "    current_date += datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe5512b-2bae-4fbf-9792-f798ebefa2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "for c in idx_calls:\n",
    "    print(c)\n",
    "\n",
    "with open('idx_calls_v4.txt', 'w') as f:\n",
    "    f.write(captured_output.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad0d93-02c2-4795-ae4a-0966225cfab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save idx_calls to file\n",
    "with open('idx_calls_v4.pkl', 'wb') as f:\n",
    "    pickle.dump(idx_calls, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd9ca5-ca24-4f90-afc5-b61d0c3b97e3",
   "metadata": {},
   "source": [
    "## Do conversion from netCDF files to IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8bf6a5-0d2c-4b39-a2be-fbfb80a13dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'len(idx_calls) = {len(idx_calls)}')\n",
    "idx_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e63f9-b881-488b-a7cb-bfa7ccf9872e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create idx file of i'th dataset\n",
    "# useful for dealing with fields that are not all the same size:\n",
    "# https://github.com/sci-visus/OpenVisus/blob/master/Samples/jupyter/nasa_conversion_example.ipynb\n",
    "\n",
    "# create OpenVisus field for the pm25 variable\n",
    "f = Field('PM25', 'float32')\n",
    "\n",
    "# create the idx file for this dataset using field f\n",
    "# dims is maximum array size, we will resample data accordingly to fit this\n",
    "# time is number of files * 24 (hours)\n",
    "db = CreateIdx(url=idx_dir + '/firesmoke.idx', fields=[f], \n",
    "               dims=[int(max_ncols[ids[0]]), int(max_nrows[ids[0]])], time=[0, len(idx_calls) - 1, '%00000000d/'])\n",
    "\n",
    "# to track what timestep we are on in idx\n",
    "tstep = 0\n",
    "\n",
    "# threshold to use to change small-enough resampled values to 0\n",
    "thresh = 1e-15\n",
    "\n",
    "for call in tqdm(idx_calls):\n",
    "    # get instructions from call\n",
    "    # [curr_id, file_str, parse_tflag(ds['TFLAG'].values[tstep_idx][0]), tstep_idx]\n",
    "    curr_id = call[0]\n",
    "    curr_file = call[1]\n",
    "    tstep_index = call[3]\n",
    "    # open the file with xarray\n",
    "    ds = xr.open_dataset(f'{firesmoke_dir}/{curr_id}/{curr_file}')\n",
    "    \n",
    "    # Get the PM25 values, squeeze out empty axis\n",
    "    file_vals = np.squeeze(ds['PM25'].values)\n",
    "    \n",
    "    # to decide if we need to resample or not\n",
    "    resamp = ds.XORIG != max_xorig\n",
    "    \n",
    "    # resample data if not already on max lat/lon grid\n",
    "    if resamp:\n",
    "        # Perform the interpolation\n",
    "        file_vals_resamp = griddata(sml_tups, file_vals[tstep_index].flatten(), big_tups, method='cubic', fill_value=0)\n",
    "        \n",
    "        # Any values that are less than a given threshold, make it 0\n",
    "        file_vals_resamp[file_vals_resamp < thresh] = 0\n",
    "        \n",
    "        # Reshape the result to match the new grid shape\n",
    "        file_vals_resamp = file_vals_resamp.reshape((len(big_lat), len(big_lon)))\n",
    "        # Write resampled values at hour h to timestep t and field f\n",
    "        db.write(data=file_vals_resamp.astype(np.float32),field=f,time=tstep)\n",
    "    else:\n",
    "        # Write original values at hour h to timestep t and field f\n",
    "        db.write(data=file_vals[tstep_index], field=f, time=tstep)\n",
    "\n",
    "    # move to next timestep in IDX\n",
    "    tstep = tstep + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b908337-8454-4f24-9cec-4325ad74f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to idx data directory\n",
    "os.chdir(idx_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3af4da-d469-44b9-9c90-20216304c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress dataset\n",
    "db.compressDataset(['zip'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
