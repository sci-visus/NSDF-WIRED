{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca8ac1b-4eaa-478a-ad7e-1364e6c1791e",
   "metadata": {},
   "source": [
    "# Firesmoke Data Conversion to IDX using OpenVisus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f4506-47b9-452e-908e-15e532e1b801",
   "metadata": {},
   "source": [
    "## Import necessary libraries, install them if you do not have them. This was developed in Python 3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f499004e-bebd-4abf-8885-f16ce6ad9531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used to read/manipulate netCDF data\n",
    "import xarray as xr\n",
    "\n",
    "# Used to convert to .idx\n",
    "from OpenVisus import *\n",
    "\n",
    "# Used for numerical work\n",
    "import numpy as np\n",
    "\n",
    "# Used for processing netCDF time data\n",
    "import datetime\n",
    "\n",
    "# Used for interacting with OS file system (to get directory file names)\n",
    "import os\n",
    "\n",
    "# To load/save final sequence array to file\n",
    "import pickle\n",
    "\n",
    "# Used for resampling arrays to fit the same lat/lon grid\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# for checking and using timestamps\n",
    "import pandas as pd\n",
    "\n",
    "# Accessory, used to generate progress bar for running for loops\n",
    "# from tqdm.notebook import tqdm\n",
    "# import ipywidgets\n",
    "# import jupyterlab_widgets\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3169e-d1dd-4050-811a-6fd661442c81",
   "metadata": {},
   "source": [
    "## Get relevant directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b0c7861-34fb-464b-95b1-4a1aa4a8ba21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ******* THIS IS WHEN RUNNING FROM LOCAL MACHINE (canada1) **************\n",
    "# path to all original netCDF files from UBC\n",
    "firesmoke_dir = \"/opt/wired-data/firesmoke/final_union_set\"\n",
    "\n",
    "# path to save idx file and data\n",
    "idx_dir = \"/opt/wired-data/firesmoke/idx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32528eee-cd60-44f6-9e14-cae79c5ce478",
   "metadata": {},
   "source": [
    "## Gather information about the metadata of our files, since it is inconsistent file to file. We need to know what values to keep consistent across all files.\n",
    "<h3 style=\"white-space: pre-line;\">\n",
    "1. Count number of files there are per firesmoke directory.\n",
    "2. Determine maximum row,col dimension sizes for pm25 array.\n",
    "3. Determine maximum latitude longitude grid parameters.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c92053f-9e91-4e6d-b46b-a9efa04c6d58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 190.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# List of all files that are available from UBC\n",
    "successful_files = np.array([])\n",
    "\n",
    "# Track all unique sizes of netCDF variables across all files\n",
    "# creating a dict of variables for each file that has differing attributes\n",
    "all_unique_dims = {}\n",
    "\n",
    "# get list of netcdf file names, in alphabetical order\n",
    "file_names = sorted(os.listdir(firesmoke_dir[0:100]))\n",
    "\n",
    "# all_unique_dims starts empty\n",
    "all_unique_dims_empty = 1\n",
    "\n",
    "# try opening each file, process only if it successfully opens\n",
    "for file in tqdm(file_names[0:200]):\n",
    "    # get file's path\n",
    "    path = f'{firesmoke_dir}/{file}'\n",
    "    \n",
    "    # keep track of which files successfully open\n",
    "    try:\n",
    "        # open the file with xarray\n",
    "        ds = xr.open_dataset(path)\n",
    "\n",
    "        # append file name to successful_files\n",
    "        successful_files = np.append(successful_files,file)\n",
    "\n",
    "        # populate all_unique_attr dict upon first successful file opening\n",
    "        if all_unique_dims_empty:\n",
    "            all_unique_dims[0] = [ds.sizes, ds.attrs]\n",
    "            all_unique_dims_empty = 0\n",
    "\n",
    "        # check if this file has unique attrs different from what we've already tracked\n",
    "        need_new_key = 1\n",
    "        for unique_key in all_unique_dims.keys():\n",
    "            if ds.sizes == all_unique_dims[unique_key][0]:\n",
    "                # we've already recorded this unique size\n",
    "                need_new_key = 0\n",
    "                continue \n",
    "\n",
    "        # add a new entry for new size\n",
    "        if need_new_key:    \n",
    "            new_key = len(all_unique_dims.keys())\n",
    "            all_unique_dims[new_key] = [ds.sizes, ds.attrs]\n",
    "        \n",
    "    except:\n",
    "        # netcdf file does not exist\n",
    "        continue\n",
    "\n",
    "# Sort list of successful files so they're in order of date\n",
    "successful_files = np.sort(successful_files).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dabd5e0",
   "metadata": {},
   "source": [
    "### Create resampling grids to resample values on smaller grid to larger grid during conversion to IDX.\n",
    "By visual inspection of `all_unique_dims` we see there is only 2 unique grid sizes used, we will refer to these as `max_grid` and `min_grid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b9fe211-ccb8-46df-93e9-92f7d18e702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max_grid and min_grid to unwrapped and merged ds.sizes and ds.attr dicts\n",
    "last_key = len(all_unique_dims.keys())-1\n",
    "max_grid = {**dict(all_unique_dims[1][0]), **dict(all_unique_dims[1][1])}\n",
    "min_grid = {**dict(all_unique_dims[last_key][0]), **dict(all_unique_dims[last_key][1])}\n",
    "\n",
    "# swap if they're incorrectly set\n",
    "if max_grid['ROW'] * max_grid['COL'] < min_grid['ROW'] * min_grid['COL']:\n",
    "    tmp = min_grid\n",
    "    min_grid = max_grid\n",
    "    max_grid = tmp\n",
    "\n",
    "# get arrays of bigger lat/lon grid\n",
    "big_lon = np.linspace(max_grid['XORIG'], max_grid['XORIG'] + max_grid['XCELL'] * (max_grid['COL'] - 1), max_grid['COL'])\n",
    "big_lat = np.linspace(max_grid['YORIG'], max_grid['YORIG'] + max_grid['YCELL'] * (max_grid['ROW'] - 1), max_grid['ROW'])\n",
    "\n",
    "# get coordinates made of new lat/lon arrays\n",
    "big_lon_pts, big_lat_pts = np.meshgrid(big_lon, big_lat)\n",
    "big_tups = np.array([tup for tup in zip(big_lon_pts.flatten(), big_lat_pts.flatten())])\n",
    "\n",
    "# get arrays of smaller lat/lon grid\n",
    "sml_lon = np.linspace(min_grid['XORIG'], min_grid['XORIG'] + min_grid['XCELL'] * (min_grid['COL'] - 1), min_grid['COL'])\n",
    "sml_lat = np.linspace(min_grid['YORIG'], min_grid['YORIG'] + min_grid['YCELL'] * (min_grid['ROW'] - 1), min_grid['ROW'])\n",
    "\n",
    "# get coordinates made of small lat/lon arrays\n",
    "sml_lon_pts, sml_lat_pts = np.meshgrid(sml_lon, sml_lat)\n",
    "sml_tups = np.array([tup for tup in zip(sml_lon_pts.flatten(), sml_lat_pts.flatten())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b68ee49-dabf-4258-95b9-707814280028",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TESTING `resample_array` AND SCRIBBLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc424296-d880-462f-921b-d4966025e1d2",
   "metadata": {},
   "source": [
    "### This is plotting the oiginal 381x1041 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30986a19-aa82-4fd3-91fb-b76ed6811a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the PM25 values, squeeze out empty axis\n",
    "# vals = np.squeeze(sml_ds['PM25'].values)\n",
    "\n",
    "# # Perform the interpolation\n",
    "# arr = griddata(sml_tups, vals[15].flatten(), big_tups, method='cubic', fill_value=0)\n",
    "\n",
    "# # Any values that are less than a given threshold, make it 0\n",
    "# arr[arr < 1e-15] = 0\n",
    "\n",
    "# # Reshape the result to match the new grid shape\n",
    "# arr = arr.reshape((len(big_lat), len(big_lon)))\n",
    "\n",
    "# arr = arr.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f64beb-b62d-46b5-b7e2-1cf1d7796449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.min(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0975b-23c0-40f1-8666-ea9579b7d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(arr[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb1f2db-95a9-467b-af10-6f671f4f48d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's use matplotlib's imshow, since our data is on a grid\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # Initialize a figure and plot, so we can customize figure and plot of data\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/getting_started/index.html\n",
    "# my_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "\n",
    "# # Let's set some parameters to get the visualization we want\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # color PM25 values on a log scale, since values are small\n",
    "# my_norm = \"log\" \n",
    "# # this will number our x and y axes based on the longitude latitude range\n",
    "# my_extent = [np.min(sml_lon), np.max(sml_lon), np.min(sml_lat), np.max(sml_lat)]\n",
    "# # ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\n",
    "# my_aspect = 'auto'\n",
    "# # tell matplotlib, our origin is the lower-left corner\n",
    "# my_origin = 'lower'\n",
    "# # select a colormap for our plot and the color bar on the right\n",
    "# my_cmap = 'viridis'\n",
    "\n",
    "# # create our plot using imshow\n",
    "# plot = my_plt.imshow(arr, norm=my_norm, extent=my_extent, \n",
    "#           aspect=my_aspect, origin=my_origin, cmap=my_cmap)\n",
    "\n",
    "# # draw coastlines\n",
    "# my_plt.coastlines()\n",
    "\n",
    "# # draw latitude longitude lines\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\n",
    "# my_plt.gridlines(draw_labels=True)\n",
    "\n",
    "# # add a colorbar to our figure, based on the plot we just made above\n",
    "# my_fig.colorbar(plot,location='right', label='ug/m^3')\n",
    "\n",
    "# # # Set x and y axis labels on our ax\n",
    "# # my_plt.set_xlabel('Longitude')\n",
    "# # my_plt.set_ylabel('Latitude')\n",
    "\n",
    "# # Set title of our figure\n",
    "# my_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n",
    "\n",
    "# # # Set title of our plot as the timestamp of our data\n",
    "# # my_plt.set_title(f'{my_timestamp}')\n",
    "\n",
    "# # Show the resulting visualization\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d18dc69-01cc-4fa9-91a5-1377bcdc6bea",
   "metadata": {},
   "source": [
    "### This is visualizing the resampled version of array above, from 381x1041 -> 381x1081 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59925a-f689-4c9f-a9a9-a897f45372e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's use matplotlib's imshow, since our data is on a grid\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # Initialize a figure and plot, so we can customize figure and plot of data\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/getting_started/index.html\n",
    "# my_fig, my_plt = plt.subplots(figsize=(15, 6), subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "\n",
    "# # Let's set some parameters to get the visualization we want\n",
    "# # ref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "\n",
    "# # color PM25 values on a log scale, since values are small\n",
    "# my_norm = \"log\" \n",
    "# # this will number our x and y axes based on the longitude latitude range\n",
    "# my_extent = [np.min(big_lon), np.max(big_lon), np.min(big_lat), np.max(big_lat)]\n",
    "# # ensure the aspect ratio of our plot fits all data, matplotlib can does this automatically\n",
    "# my_aspect = 'auto'\n",
    "# # tell matplotlib, our origin is the lower-left corner\n",
    "# my_origin = 'lower'\n",
    "# # select a colormap for our plot and the color bar on the right\n",
    "# my_cmap = 'viridis'\n",
    "\n",
    "# # create our plot using imshow\n",
    "# plot = my_plt.imshow(arr_resamp, norm=my_norm, extent=my_extent, \n",
    "#           aspect=my_aspect, origin=my_origin, cmap=my_cmap, vmin=.00001, vmax=1)\n",
    "\n",
    "# # draw coastlines\n",
    "# my_plt.coastlines()\n",
    "\n",
    "# # draw latitude longitude lines\n",
    "# # ref: https://scitools.org.uk/cartopy/docs/latest/gallery/gridlines_and_labels/gridliner.html\n",
    "# my_plt.gridlines(draw_labels=True)\n",
    "\n",
    "# # add a colorbar to our figure, based on the plot we just made above\n",
    "# my_fig.colorbar(plot,location='right', label='ug/m^3')\n",
    "\n",
    "# # # Set x and y axis labels on our ax\n",
    "# # my_plt.set_xlabel('Longitude')\n",
    "# # my_plt.set_ylabel('Latitude')\n",
    "\n",
    "# # Set title of our figure\n",
    "# my_fig.suptitle('Ground level concentration of PM2.5 microns and smaller')\n",
    "\n",
    "# # # Set title of our plot as the timestamp of our data\n",
    "# # my_plt.set_title(f'{my_timestamp}')\n",
    "\n",
    "# # Show the resulting visualization\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f939c8-a3f9-4ec5-ab38-6c6cae22d486",
   "metadata": {},
   "source": [
    "## Determine sequence of files to load later for IDX conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb53b83-197f-4af8-ba35-118c50d5b699",
   "metadata": {},
   "source": [
    "### First determine what hours are available in all datasets, from there we construct final sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f7e05fb-8237-41cf-9b35-e7c25aed27c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for parsing time flags (TFLAG) from netcdf files\n",
    "# return datetime object of tflag\n",
    "# param [int, int] tflag: the TFLAG to parse and return as datetime object \n",
    "def parse_tflag(tflag):\n",
    "    year = int(tflag[0] // 1000)\n",
    "    day_of_year = int(tflag[0] % 1000)\n",
    "    date = datetime.datetime(year, 1, 1) + datetime.timedelta(days=day_of_year - 1)\n",
    "\n",
    "    time_in_day = int(tflag[1])\n",
    "    hours = time_in_day // 10000\n",
    "    minutes = (time_in_day % 10000) // 100\n",
    "    seconds = time_in_day % 100\n",
    "\n",
    "    full_datetime = datetime.datetime(year, date.month, date.day, hours, minutes, seconds)\n",
    "    return full_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33f92213-d818-425b-a39c-e94a7b89e875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 282.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# get set of all available hours from successful_files\n",
    "# we use a dictionary so we can index by CDATE\n",
    "available_dates = {np.int32(file.split('_')[1]): {} for file in successful_files}\n",
    "\n",
    "rows = []\n",
    "\n",
    "for file in tqdm(successful_files):\n",
    "    # get file's path\n",
    "    path = f'{firesmoke_dir}/{file}'\n",
    "    \n",
    "    # open the file with xarray\n",
    "    ds = xr.open_dataset(path)\n",
    "\n",
    "    # for each CDATE_CTIME, store their respective TFLAGs\n",
    "    cdatetime = pd.to_datetime(f\"{ds.CDATE}_{ds.CTIME}\", format='%Y%j_%H%M%S')\n",
    "    tflags = ds['TFLAG'][:, 0, :].values\n",
    "\n",
    "    # append new row of CDATETIMEs with their respective TFLAGs\n",
    "    rows.append({\n",
    "            'CDATETIME': cdatetime,\n",
    "            'TFLAG': tflags\n",
    "        })\n",
    "available_dates_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "422b5bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CDATETIME</th>\n",
       "      <th>TFLAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-28 02:50:55</td>\n",
       "      <td>[[2021059, 30000], [2021059, 40000], [2021059,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-28 12:00:47</td>\n",
       "      <td>[[2021059, 90000], [2021059, 100000], [2021059...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-28 14:49:58</td>\n",
       "      <td>[[2021059, 150000], [2021059, 160000], [202105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-28 20:52:38</td>\n",
       "      <td>[[2021059, 210000], [2021059, 220000], [202105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-01 02:52:15</td>\n",
       "      <td>[[2021060, 30000], [2021060, 40000], [2021060,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2021-04-20 03:07:46</td>\n",
       "      <td>[[2021110, 30000], [2021110, 40000], [2021110,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2021-04-20 11:31:58</td>\n",
       "      <td>[[2021110, 90000], [2021110, 100000], [2021110...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2021-04-20 15:15:08</td>\n",
       "      <td>[[2021110, 150000], [2021110, 160000], [202111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2021-04-20 21:07:47</td>\n",
       "      <td>[[2021110, 210000], [2021110, 220000], [202111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2021-04-21 03:08:06</td>\n",
       "      <td>[[2021111, 30000], [2021111, 40000], [2021111,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              CDATETIME                                              TFLAG\n",
       "0   2021-02-28 02:50:55  [[2021059, 30000], [2021059, 40000], [2021059,...\n",
       "1   2021-02-28 12:00:47  [[2021059, 90000], [2021059, 100000], [2021059...\n",
       "2   2021-02-28 14:49:58  [[2021059, 150000], [2021059, 160000], [202105...\n",
       "3   2021-02-28 20:52:38  [[2021059, 210000], [2021059, 220000], [202105...\n",
       "4   2021-03-01 02:52:15  [[2021060, 30000], [2021060, 40000], [2021060,...\n",
       "..                  ...                                                ...\n",
       "195 2021-04-20 03:07:46  [[2021110, 30000], [2021110, 40000], [2021110,...\n",
       "196 2021-04-20 11:31:58  [[2021110, 90000], [2021110, 100000], [2021110...\n",
       "197 2021-04-20 15:15:08  [[2021110, 150000], [2021110, 160000], [202111...\n",
       "198 2021-04-20 21:07:47  [[2021110, 210000], [2021110, 220000], [202111...\n",
       "199 2021-04-21 03:08:06  [[2021111, 30000], [2021111, 40000], [2021111,...\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_dates_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c91637a-b19b-44b0-8f4d-0023a0cb2334",
   "metadata": {},
   "source": [
    "Step through all hours we want to represent (earliest available firesmoke file's CDATE to present day) and grab from according dispersion_[CDATE]_[CTIME].nc file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23c8a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_idx_calls(arr, cdatetime, tstep_idx):\n",
    "    '''\n",
    "    For the given array, append arguments specifying which dispersion.nc file to open\n",
    "        and which TFLAG to use as found in dates_df\n",
    "    :param list arr: array that holds final idx write sequence\n",
    "    :param str cdatetime: datetime object of CDATE + CTIME of the dispersion file we will open\n",
    "    :param datetime time_idx: The index of the TFLAG in TFLAG array we will select from dispersion file\n",
    "    '''\n",
    "    # get path of file we will use\n",
    "    cdate_str = cdatetime.date().strftime(\"%Y%j\")\n",
    "    ctime_str = cdatetime.time().strftime(\"%H%M%S\")\n",
    "    file_str = f\"dispersion_{cdate_str}_{ctime_str}.nc\"\n",
    "    path = f'{firesmoke_dir}/{file_str}'\n",
    "\n",
    "    # open the file with xarray\n",
    "    ds = xr.open_dataset(path)\n",
    "    arr.append([file_str, parse_tflag(ds['TFLAG'].values[tstep_idx][0]), tstep_idx])\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dbe72c",
   "metadata": {},
   "source": [
    "## **TODO**: Make sure the `cdate` and `ctime` you're selecting is actually correct... as shown below for example, can a file 'made in the future' give the latest forecast prediction for a timestep 'in the past'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c61b4235-7d40-49f6-b6d2-aeaff5e0cdab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: No available file found for current_hour 2021-02-28 00:00:00 (date: 2021-02-28 00:00:00). Skipping.\n",
      "~~~~~\n",
      "WARNING: No available file found for current_hour 2021-02-28 01:00:00 (date: 2021-02-28 00:00:00). Skipping.\n",
      "~~~~~\n",
      "WARNING: No available file found for current_hour 2021-02-28 02:00:00 (date: 2021-02-28 00:00:00). Skipping.\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 03:00:00: using CDATE=2021-02-28 02:50:55\n",
      "tflag_0 = 2021-02-28 03:00:00, current_hour = 2021-02-28 03:00:00, tstep_diff = 0\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 04:00:00: using CDATE=2021-02-28 02:50:55\n",
      "tflag_0 = 2021-02-28 03:00:00, current_hour = 2021-02-28 04:00:00, tstep_diff = 1\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 05:00:00: using CDATE=2021-02-28 02:50:55\n",
      "tflag_0 = 2021-02-28 03:00:00, current_hour = 2021-02-28 05:00:00, tstep_diff = 2\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 06:00:00: using CDATE=2021-02-28 02:50:55\n",
      "tflag_0 = 2021-02-28 03:00:00, current_hour = 2021-02-28 06:00:00, tstep_diff = 3\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 07:00:00: using CDATE=2021-02-28 02:50:55\n",
      "tflag_0 = 2021-02-28 03:00:00, current_hour = 2021-02-28 07:00:00, tstep_diff = 4\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 08:00:00: using CDATE=2021-02-28 02:50:55\n",
      "tflag_0 = 2021-02-28 03:00:00, current_hour = 2021-02-28 08:00:00, tstep_diff = 5\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 09:00:00: using CDATE=2021-02-28 02:50:55\n",
      "tflag_0 = 2021-02-28 03:00:00, current_hour = 2021-02-28 09:00:00, tstep_diff = 6\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 10:00:00: using CDATE=2021-02-28 02:50:55\n",
      "tflag_0 = 2021-02-28 03:00:00, current_hour = 2021-02-28 10:00:00, tstep_diff = 7\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 11:00:00: using CDATE=2021-02-28 02:50:55\n",
      "tflag_0 = 2021-02-28 03:00:00, current_hour = 2021-02-28 11:00:00, tstep_diff = 8\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 12:00:00: using CDATE=2021-02-28 02:50:55\n",
      "tflag_0 = 2021-02-28 03:00:00, current_hour = 2021-02-28 12:00:00, tstep_diff = 9\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 13:00:00: using CDATE=2021-02-28 12:00:47\n",
      "tflag_0 = 2021-02-28 09:00:00, current_hour = 2021-02-28 13:00:00, tstep_diff = 4\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 14:00:00: using CDATE=2021-02-28 12:00:47\n",
      "tflag_0 = 2021-02-28 09:00:00, current_hour = 2021-02-28 14:00:00, tstep_diff = 5\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 15:00:00: using CDATE=2021-02-28 14:49:58\n",
      "tflag_0 = 2021-02-28 15:00:00, current_hour = 2021-02-28 15:00:00, tstep_diff = 0\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 16:00:00: using CDATE=2021-02-28 14:49:58\n",
      "tflag_0 = 2021-02-28 15:00:00, current_hour = 2021-02-28 16:00:00, tstep_diff = 1\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 17:00:00: using CDATE=2021-02-28 14:49:58\n",
      "tflag_0 = 2021-02-28 15:00:00, current_hour = 2021-02-28 17:00:00, tstep_diff = 2\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 18:00:00: using CDATE=2021-02-28 14:49:58\n",
      "tflag_0 = 2021-02-28 15:00:00, current_hour = 2021-02-28 18:00:00, tstep_diff = 3\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 19:00:00: using CDATE=2021-02-28 14:49:58\n",
      "tflag_0 = 2021-02-28 15:00:00, current_hour = 2021-02-28 19:00:00, tstep_diff = 4\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 20:00:00: using CDATE=2021-02-28 14:49:58\n",
      "tflag_0 = 2021-02-28 15:00:00, current_hour = 2021-02-28 20:00:00, tstep_diff = 5\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 21:00:00: using CDATE=2021-02-28 20:52:38\n",
      "tflag_0 = 2021-02-28 21:00:00, current_hour = 2021-02-28 21:00:00, tstep_diff = 0\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 22:00:00: using CDATE=2021-02-28 20:52:38\n",
      "tflag_0 = 2021-02-28 21:00:00, current_hour = 2021-02-28 22:00:00, tstep_diff = 1\n",
      "~~~~~\n",
      "Found data for current_hour 2021-02-28 23:00:00: using CDATE=2021-02-28 20:52:38\n",
      "tflag_0 = 2021-02-28 21:00:00, current_hour = 2021-02-28 23:00:00, tstep_diff = 2\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 00:00:00: using CDATE=2021-02-28 20:52:38\n",
      "tflag_0 = 2021-02-28 21:00:00, current_hour = 2021-03-01 00:00:00, tstep_diff = 3\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 01:00:00: using CDATE=2021-02-28 20:52:38\n",
      "tflag_0 = 2021-02-28 21:00:00, current_hour = 2021-03-01 01:00:00, tstep_diff = 4\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 02:00:00: using CDATE=2021-02-28 20:52:38\n",
      "tflag_0 = 2021-02-28 21:00:00, current_hour = 2021-03-01 02:00:00, tstep_diff = 5\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 03:00:00: using CDATE=2021-03-01 02:52:15\n",
      "tflag_0 = 2021-03-01 03:00:00, current_hour = 2021-03-01 03:00:00, tstep_diff = 0\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 04:00:00: using CDATE=2021-03-01 02:52:15\n",
      "tflag_0 = 2021-03-01 03:00:00, current_hour = 2021-03-01 04:00:00, tstep_diff = 1\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 05:00:00: using CDATE=2021-03-01 02:52:15\n",
      "tflag_0 = 2021-03-01 03:00:00, current_hour = 2021-03-01 05:00:00, tstep_diff = 2\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 06:00:00: using CDATE=2021-03-01 02:52:15\n",
      "tflag_0 = 2021-03-01 03:00:00, current_hour = 2021-03-01 06:00:00, tstep_diff = 3\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 07:00:00: using CDATE=2021-03-01 02:52:15\n",
      "tflag_0 = 2021-03-01 03:00:00, current_hour = 2021-03-01 07:00:00, tstep_diff = 4\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 08:00:00: using CDATE=2021-03-01 02:52:15\n",
      "tflag_0 = 2021-03-01 03:00:00, current_hour = 2021-03-01 08:00:00, tstep_diff = 5\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 09:00:00: using CDATE=2021-03-01 02:52:15\n",
      "tflag_0 = 2021-03-01 03:00:00, current_hour = 2021-03-01 09:00:00, tstep_diff = 6\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 10:00:00: using CDATE=2021-03-01 02:52:15\n",
      "tflag_0 = 2021-03-01 03:00:00, current_hour = 2021-03-01 10:00:00, tstep_diff = 7\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 11:00:00: using CDATE=2021-03-01 02:52:15\n",
      "tflag_0 = 2021-03-01 03:00:00, current_hour = 2021-03-01 11:00:00, tstep_diff = 8\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 12:00:00: using CDATE=2021-03-01 02:52:15\n",
      "tflag_0 = 2021-03-01 03:00:00, current_hour = 2021-03-01 12:00:00, tstep_diff = 9\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 13:00:00: using CDATE=2021-03-01 02:52:15\n",
      "tflag_0 = 2021-03-01 03:00:00, current_hour = 2021-03-01 13:00:00, tstep_diff = 10\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 14:00:00: using CDATE=2021-03-01 13:45:50\n",
      "tflag_0 = 2021-03-01 09:00:00, current_hour = 2021-03-01 14:00:00, tstep_diff = 5\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 15:00:00: using CDATE=2021-03-01 13:45:50\n",
      "tflag_0 = 2021-03-01 09:00:00, current_hour = 2021-03-01 15:00:00, tstep_diff = 6\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 16:00:00: using CDATE=2021-03-01 15:12:22\n",
      "tflag_0 = 2021-03-01 15:00:00, current_hour = 2021-03-01 16:00:00, tstep_diff = 1\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 17:00:00: using CDATE=2021-03-01 15:12:22\n",
      "tflag_0 = 2021-03-01 15:00:00, current_hour = 2021-03-01 17:00:00, tstep_diff = 2\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 18:00:00: using CDATE=2021-03-01 15:12:22\n",
      "tflag_0 = 2021-03-01 15:00:00, current_hour = 2021-03-01 18:00:00, tstep_diff = 3\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 19:00:00: using CDATE=2021-03-01 15:12:22\n",
      "tflag_0 = 2021-03-01 15:00:00, current_hour = 2021-03-01 19:00:00, tstep_diff = 4\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 20:00:00: using CDATE=2021-03-01 15:12:22\n",
      "tflag_0 = 2021-03-01 15:00:00, current_hour = 2021-03-01 20:00:00, tstep_diff = 5\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 21:00:00: using CDATE=2021-03-01 20:52:28\n",
      "tflag_0 = 2021-03-01 21:00:00, current_hour = 2021-03-01 21:00:00, tstep_diff = 0\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 22:00:00: using CDATE=2021-03-01 20:52:28\n",
      "tflag_0 = 2021-03-01 21:00:00, current_hour = 2021-03-01 22:00:00, tstep_diff = 1\n",
      "~~~~~\n",
      "Found data for current_hour 2021-03-01 23:00:00: using CDATE=2021-03-01 20:52:28\n",
      "tflag_0 = 2021-03-01 21:00:00, current_hour = 2021-03-01 23:00:00, tstep_diff = 2\n",
      "~~~~~\n"
     ]
    }
   ],
   "source": [
    "# Arrays to hold the final order we will index files\n",
    "from datetime import timedelta\n",
    "\n",
    "idx_calls = []\n",
    "\n",
    "# Define the start and end dates we will step through\n",
    "# start_date = datetime.datetime.strptime(\"2021059\", \"%Y%j\")\n",
    "# end_date = datetime.datetime.strptime(\"2025317\", \"%Y%j\")\n",
    "\n",
    "# TMP DATES\n",
    "start_date = datetime.datetime.strptime(\"2021059\", \"%Y%j\")\n",
    "end_date = datetime.datetime.strptime(\"2021060\", \"%Y%j\")\n",
    "\n",
    "# iterate over each day\n",
    "current_date = start_date\n",
    "# iterate over each hour of the current day\n",
    "current_hour = datetime.datetime(current_date.year, current_date.month, current_date.day)\n",
    "\n",
    "# file to open\n",
    "file_str = ''\n",
    "\n",
    "# tell functions to print for debugging\n",
    "verbose = 1\n",
    "\n",
    "while current_date <= end_date:    \n",
    "    while current_hour < current_date + datetime.timedelta(days=1):\n",
    "        # set search counter\n",
    "        found = 0\n",
    "        prev_day_count = 0\n",
    "        most_recent_row = None\n",
    "\n",
    "        # search files starting from current_date and \n",
    "        # continue if data point is not found for current_hour, current_date\n",
    "        # end search after searching previous 4 days\n",
    "        while found == 0 and prev_day_count <= 4:\n",
    "            # Select rows that have CDATETIME created at most 4 days before or at same time as current_hour\n",
    "            mask_is_valid_date = (available_dates_df['CDATETIME'] >= (current_hour - datetime.timedelta(days=4))) & (available_dates_df['CDATETIME'] <= current_hour)\n",
    "            dt_mask = mask_is_valid_date\n",
    "\n",
    "            # If such a row exists, select the row with closest CDATETIME to current_hour\n",
    "            if available_dates_df[dt_mask].shape[0] > 0:\n",
    "                most_recent_row = available_dates_df[dt_mask].iloc[-1]\n",
    "                found = 1\n",
    "            else:\n",
    "                prev_day_count += 1\n",
    "        \n",
    "        # if we found a row, update idx_calls array to specify we will use:\n",
    "        # TFLAG[tstep_idx] from the dispersion file named with CDATETIME to represent current_hour in our final IDX file\n",
    "        if found:\n",
    "            # the number of hours difference between 0th TFLAG and current_hour is tstep_idx\n",
    "            tflag_0 = parse_tflag(most_recent_row['TFLAG'][0])\n",
    "            tstep_idx = int((current_hour - tflag_0).total_seconds() / 3600)\n",
    "\n",
    "            if verbose:\n",
    "                print(f'Found data for current_hour {current_hour}: using CDATE={most_recent_row[\"CDATETIME\"]}')\n",
    "                print(f\"tflag_0 = {tflag_0}, current_hour = {current_hour}, tstep_diff = {tstep_idx}\")\n",
    "\n",
    "            # update idx call array\n",
    "            update_idx_calls(idx_calls, most_recent_row['CDATETIME'], tstep_idx)\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f'WARNING: No available file found for current_hour {current_hour} (date: {current_date}). Skipping.')\n",
    "\n",
    "        # move to next hour\n",
    "        current_hour += datetime.timedelta(hours=1)\n",
    "\n",
    "        print('~~~~~')\n",
    "\n",
    "    # move to the next day\n",
    "    current_date += datetime.timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4fe5512b-2bae-4fbf-9792-f798ebefa2c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "for c in idx_calls:\n",
    "    print(c)\n",
    "\n",
    "with open('idx_calls_v5.txt', 'w') as f:\n",
    "    f.write(captured_output.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8bad0d93-02c2-4795-ae4a-0966225cfab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save idx_calls to file\n",
    "with open('idx_calls_v5.pkl', 'wb') as f:\n",
    "    pickle.dump(idx_calls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ac22ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispersion_2021059_025055.nc', datetime.datetime(2021, 2, 28, 3, 0), 0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_calls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd9ca5-ca24-4f90-afc5-b61d0c3b97e3",
   "metadata": {},
   "source": [
    "## Do conversion from netCDF files to IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c8bf6a5-0d2c-4b39-a2be-fbfb80a13dea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(idx_calls) = 45\n"
     ]
    }
   ],
   "source": [
    "print(f'len(idx_calls) = {len(idx_calls)}')\n",
    "# idx_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a96e63f9-b881-488b-a7cb-bfa7ccf9872e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [04:08<00:00,  5.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create idx file of i'th dataset\n",
    "# useful for dealing with fields that are not all the same size:\n",
    "# https://github.com/sci-visus/OpenVisus/blob/master/Samples/jupyter/nasa_conversion_example.ipynb\n",
    "\n",
    "# create OpenVisus field for the pm25 variable\n",
    "f = Field('PM25', 'float32')\n",
    "\n",
    "# create the idx file for this dataset using field f\n",
    "# dims is maximum array size, we will resample data accordingly to fit this\n",
    "# time is number of files * 24 (hours)\n",
    "db = CreateIdx(url=idx_dir + '/firesmoke.idx', fields=[f], \n",
    "               dims=[int(max_grid['COL']), int(max_grid['ROW'])], time=[0, len(idx_calls) - 1, '%00000000d/'])\n",
    "\n",
    "# to track what timestep we are on in IDX conversion\n",
    "tstep = 0\n",
    "\n",
    "# threshold to use to change small-enough resampled values to 0\n",
    "thresh = 1e-15\n",
    "\n",
    "for call in tqdm(idx_calls):\n",
    "    # get instructions from call:\n",
    "    # [file name to open, timestamp, TSTEP index to select]\n",
    "    file_name = call[0]\n",
    "    timestamp = call[1]\n",
    "    tstep_index = call[2]\n",
    "    # open the file with xarray\n",
    "    ds = xr.open_dataset(f'{firesmoke_dir}/{file_name}')\n",
    "    \n",
    "    # Get the PM25 values, squeeze out empty axis\n",
    "    file_vals = np.squeeze(ds['PM25'].values)\n",
    "    \n",
    "    # to decide if we need to resample or not\n",
    "    resamp = ds.XORIG != max_grid['XORIG']\n",
    "    \n",
    "    # resample data if not already on max lat/lon grid\n",
    "    if resamp:\n",
    "        # Perform the interpolation\n",
    "        file_vals_resamp = griddata(sml_tups, file_vals[tstep_index].flatten(), big_tups, method='cubic', fill_value=0)\n",
    "        \n",
    "        # Any values that are less than a given threshold, make it 0\n",
    "        file_vals_resamp[file_vals_resamp < thresh] = 0\n",
    "        \n",
    "        # Reshape the result to match the new grid shape\n",
    "        file_vals_resamp = file_vals_resamp.reshape((len(big_lat), len(big_lon)))\n",
    "        # Write resampled values at hour h to timestep t and field f\n",
    "        db.write(data=file_vals_resamp.astype(np.float32),field=f,time=tstep)\n",
    "    else:\n",
    "        # Write original values at hour h to timestep t and field f\n",
    "        db.write(data=file_vals[tstep_index], field=f, time=tstep)\n",
    "\n",
    "    # move to next timestep in IDX\n",
    "    tstep = tstep + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b908337-8454-4f24-9cec-4325ad74f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to idx data directory\n",
    "os.chdir(idx_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d3af4da-d469-44b9-9c90-20216304c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress dataset\n",
    "db.compressDataset(['zip'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wired_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
