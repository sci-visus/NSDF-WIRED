{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca8ac1b-4eaa-478a-ad7e-1364e6c1791e",
   "metadata": {},
   "source": [
    "# Firesmoke Data Conversion to IDX using OpenVisus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f4506-47b9-452e-908e-15e532e1b801",
   "metadata": {},
   "source": [
    "## Import necessary libraries, install them if you do not have them. This was developed in Python 3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499004e-bebd-4abf-8885-f16ce6ad9531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used to read/manipulate netCDF data\n",
    "import xarray as xr\n",
    "\n",
    "# Used to convert to .idx\n",
    "from OpenVisus import *\n",
    "\n",
    "# Used for numerical work\n",
    "import numpy as np\n",
    "\n",
    "# Used for processing netCDF time data\n",
    "import datetime\n",
    "\n",
    "# Used for interacting with OS file system (to get directory file names)\n",
    "import os\n",
    "\n",
    "# To load/save final sequence array to file\n",
    "import pickle\n",
    "\n",
    "# Used for resampling arrays to fit the same lat/lon grid\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# for checking and using timestamps\n",
    "import pandas as pd\n",
    "\n",
    "# Accessory, used to generate progress bar for running for loops\n",
    "# from tqdm.notebook import tqdm\n",
    "# import ipywidgets\n",
    "# import jupyterlab_widgets\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For logging\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print for debugging\n",
    "verbose = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3169e-d1dd-4050-811a-6fd661442c81",
   "metadata": {},
   "source": [
    "## Get relevant directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c7861-34fb-464b-95b1-4a1aa4a8ba21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ******* THIS IS WHEN RUNNING FROM LOCAL MACHINE (canada1) **************\n",
    "# path to all original netCDF files from UBC\n",
    "firesmoke_dir = \"/opt/wired-data/firesmoke/final_union_set\"\n",
    "\n",
    "# path to save idx file and data\n",
    "idx_dir = \"/opt/wired-data/firesmoke/idx_parallel\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32528eee-cd60-44f6-9e14-cae79c5ce478",
   "metadata": {},
   "source": [
    "## Gather dimension sizes of all available NetCDF files (which potentially vary file to file) and files openable in xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92053f-9e91-4e6d-b46b-a9efa04c6d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ordered list of all files that are available from UBC\n",
    "successful_files = np.array([])\n",
    "\n",
    "# Track all unique sizes of netCDF variables across all files\n",
    "# creating a dict of variables for each file that has differing attributes\n",
    "all_unique_dims = {}\n",
    "\n",
    "# get list of NetCDF file names, in alphabetical (chronological) order\n",
    "file_names = sorted(os.listdir(firesmoke_dir))\n",
    "\n",
    "# all_unique_dims starts empty\n",
    "all_unique_dims_empty = 1\n",
    "\n",
    "# try opening each file, process only if it successfully opens\n",
    "for file in tqdm(file_names):\n",
    "    # get file's path\n",
    "    path = f'{firesmoke_dir}/{file}'\n",
    "\n",
    "    # keep track of which files successfully open\n",
    "    try:\n",
    "        # open the file with xarray\n",
    "        ds = xr.open_dataset(path)\n",
    "\n",
    "        # append file name to successful_files\n",
    "        successful_files = np.append(successful_files,file)\n",
    "\n",
    "        # populate all_unique_attr dict upon first successful file opening\n",
    "        if all_unique_dims_empty:\n",
    "            all_unique_dims[0] = [ds.sizes, ds.attrs]\n",
    "            all_unique_dims_empty = 0\n",
    "        \n",
    "        # check if this file has unique attrs different from what we've already tracked\n",
    "        need_new_key = 1\n",
    "        for unique_key in all_unique_dims.keys():\n",
    "            if ds.sizes == all_unique_dims[unique_key][0]:\n",
    "                # we've already recorded this unique size\n",
    "                need_new_key = 0\n",
    "                continue \n",
    "\n",
    "        # add a new entry for new size\n",
    "        if need_new_key:    \n",
    "            new_key = len(all_unique_dims.keys())\n",
    "            all_unique_dims[new_key] = [ds.sizes, ds.attrs]\n",
    "        \n",
    "    except:\n",
    "        # netcdf file does not exist\n",
    "        continue\n",
    "\n",
    "# Sort list of successful files so they're in order of date\n",
    "successful_files = np.sort(successful_files).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dabd5e0",
   "metadata": {},
   "source": [
    "### Create resampling grids to resample values on smaller grid to larger grid during conversion to IDX.\n",
    "IDX file format requires data at each timestep to be of consistent shape.\n",
    "\n",
    "By visual inspection of `all_unique_dims` we see there is only 2 unique grid sizes used, we will refer to these as `max_grid` and `min_grid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9fe211-ccb8-46df-93e9-92f7d18e702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max_grid and min_grid to unwrapped and merged ds.sizes and ds.attr dicts\n",
    "last_key = len(all_unique_dims.keys())-1\n",
    "max_grid = {**dict(all_unique_dims[1][0]), **dict(all_unique_dims[1][1])}\n",
    "min_grid = {**dict(all_unique_dims[last_key][0]), **dict(all_unique_dims[last_key][1])}\n",
    "\n",
    "# swap if they're incorrectly set\n",
    "if max_grid['ROW'] * max_grid['COL'] < min_grid['ROW'] * min_grid['COL']:\n",
    "    tmp = min_grid\n",
    "    min_grid = max_grid\n",
    "    max_grid = tmp\n",
    "\n",
    "# get arrays of bigger lat/lon grid\n",
    "big_lon = np.linspace(max_grid['XORIG'], max_grid['XORIG'] + max_grid['XCELL'] * (max_grid['COL'] - 1), max_grid['COL'])\n",
    "big_lat = np.linspace(max_grid['YORIG'], max_grid['YORIG'] + max_grid['YCELL'] * (max_grid['ROW'] - 1), max_grid['ROW'])\n",
    "\n",
    "# get coordinates made of new lat/lon arrays\n",
    "big_lon_pts, big_lat_pts = np.meshgrid(big_lon, big_lat)\n",
    "big_tups = np.array([tup for tup in zip(big_lon_pts.flatten(), big_lat_pts.flatten())])\n",
    "\n",
    "# get arrays of smaller lat/lon grid\n",
    "sml_lon = np.linspace(min_grid['XORIG'], min_grid['XORIG'] + min_grid['XCELL'] * (min_grid['COL'] - 1), min_grid['COL'])\n",
    "sml_lat = np.linspace(min_grid['YORIG'], min_grid['YORIG'] + min_grid['YCELL'] * (min_grid['ROW'] - 1), min_grid['ROW'])\n",
    "\n",
    "# get coordinates made of small lat/lon arrays\n",
    "sml_lon_pts, sml_lat_pts = np.meshgrid(sml_lon, sml_lat)\n",
    "sml_tups = np.array([tup for tup in zip(sml_lon_pts.flatten(), sml_lat_pts.flatten())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f939c8-a3f9-4ec5-ab38-6c6cae22d486",
   "metadata": {},
   "source": [
    "## Determine sequence of files to load later for IDX conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb53b83-197f-4af8-ba35-118c50d5b699",
   "metadata": {},
   "source": [
    "### First determine what hours are available in all datasets, from there we construct final sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e05fb-8237-41cf-9b35-e7c25aed27c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for parsing time flags (TFLAG) from netcdf files\n",
    "# return tflag converted to a datetime object\n",
    "# param [int, int] tflag: the TFLAG to parse and return as datetime object \n",
    "def parse_tflag(tflag):\n",
    "    year = int(tflag[0] // 1000)\n",
    "    day_of_year = int(tflag[0] % 1000)\n",
    "    date = datetime.datetime(year, 1, 1) + datetime.timedelta(days=day_of_year - 1)\n",
    "\n",
    "    time_in_day = int(tflag[1])\n",
    "    hours = time_in_day // 10000\n",
    "    minutes = (time_in_day % 10000) // 100\n",
    "    seconds = time_in_day % 100\n",
    "\n",
    "    full_datetime = datetime.datetime(year, date.month, date.day, hours, minutes, seconds)\n",
    "    return full_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f92213-d818-425b-a39c-e94a7b89e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get set of all available hours from successful_files\n",
    "# we use a dictionary so we can index by CDATE\n",
    "available_dates = {np.int32(file.split('_')[1]): {} for file in successful_files}\n",
    "\n",
    "rows = []\n",
    "\n",
    "for file in tqdm(successful_files):\n",
    "    # get file's path\n",
    "    path = f'{firesmoke_dir}/{file}'\n",
    "    \n",
    "    # open the file with xarray\n",
    "    ds = xr.open_dataset(path)\n",
    "\n",
    "    # for each CDATE_CTIME, store their respective TFLAGs\n",
    "    cdatetime = pd.to_datetime(f\"{ds.CDATE}_{ds.CTIME:06d}\", format='%Y%j_%H%M%S')\n",
    "    tflags = ds['TFLAG'][:, 0, :].values\n",
    "\n",
    "    # append new row of CDATETIMEs with their respective TFLAGs\n",
    "    rows.append({\n",
    "            'CDATETIME': cdatetime,\n",
    "            'TFLAG': tflags\n",
    "        })\n",
    "available_dates_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd383d8",
   "metadata": {},
   "source": [
    "\n",
    "## Select best NetCDF file and TSTEP[i] to represent each hour from `start_date` to `end_date` as indicated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26499adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_idx_calls(arr, cdatetime, tstep_idx):\n",
    "    '''\n",
    "    For the given array, append arguments specifying which dispersion.nc file to open\n",
    "        and which TFLAG to use as found in dates_df\n",
    "    :param list arr: array that holds final idx write sequence\n",
    "    :param str cdatetime: datetime object of CDATE + CTIME of the dispersion file we will open\n",
    "    :param datetime time_idx: The index of the TFLAG in TFLAG array we will select from dispersion file\n",
    "    '''\n",
    "\n",
    "    # get path of file we will use\n",
    "    cdate_str = cdatetime.date().strftime(\"%Y%j\")\n",
    "    ctime_str = cdatetime.time().strftime(\"%H%M%S\").zfill(6)\n",
    "    file_str = f\"dispersion_{cdate_str}_{ctime_str}.nc\"\n",
    "    path = f'{firesmoke_dir}/{file_str}'\n",
    "\n",
    "    # open the file with xarray\n",
    "    ds = xr.open_dataset(path)\n",
    "    arr.append([file_str, parse_tflag(ds['TFLAG'].values[tstep_idx][0]), tstep_idx])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea52cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up log of selected IDX files for debugging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set up logging\n",
    "# ref: https://realpython.com/python-logging/\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"/home/arleth/NSDF-WIRED/conversion/idx_calls_building_parallel.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61b4235-7d40-49f6-b6d2-aeaff5e0cdab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# verbose = 1\n",
    "# Arrays to hold the final order we will index files\n",
    "idx_calls = []\n",
    "\n",
    "# Define the start and end dates we will step through\n",
    "start_date = datetime.datetime.strptime(\"2021059\", \"%Y%j\")\n",
    "end_date = datetime.datetime.strptime(\"2025317\", \"%Y%j\")\n",
    "\n",
    "# iterate over each day\n",
    "current_date = start_date\n",
    "# iterate over each hour of the current day\n",
    "current_hour = datetime.datetime(current_date.year, current_date.month, current_date.day)\n",
    "\n",
    "# file to open\n",
    "file_str = ''\n",
    "\n",
    "while current_date <= end_date:    \n",
    "    while current_hour < current_date + datetime.timedelta(days=1):\n",
    "        # set search counter\n",
    "        found = 0\n",
    "        most_recent_row = None\n",
    "\n",
    "        # select all files where data point for current_hour may be found\n",
    "        # Select rows that have CDATETIME created at most 4 days before or at same time as current_hour\n",
    "        # Only match on the date (ignore the hour, only compare YYYYMMDD)\n",
    "        mask_is_valid_date = (\n",
    "            (available_dates_df['CDATETIME'].dt.date >= (current_hour - datetime.timedelta(days=4)).date()) &\n",
    "            (available_dates_df['CDATETIME'].dt.date <= (current_hour + datetime.timedelta(days=4)).date())\n",
    "        )\n",
    "        dt_mask = mask_is_valid_date\n",
    "\n",
    "        # If such files exists, select the row with closest CDATETIME to current_hour\n",
    "        if available_dates_df[dt_mask].shape[0] > 0:\n",
    "            curr_row = len(available_dates_df[dt_mask]) - 1\n",
    "            # ensure current hour actually exists between tflag_0 and tflag_last, otherwise, select the next most recent row\n",
    "            while curr_row >= 0 and found == 0:\n",
    "                most_recent_row = available_dates_df[dt_mask].iloc[curr_row]\n",
    "                tflag_0 = parse_tflag(most_recent_row['TFLAG'][0])\n",
    "                tflag_last = parse_tflag(most_recent_row['TFLAG'][-1])\n",
    "\n",
    "                if tflag_0 <= current_hour <= tflag_last:\n",
    "                    found = 1\n",
    "                else:\n",
    "                    curr_row -= 1\n",
    "\n",
    "        # if we found a row, update idx_calls array to specify we will use:\n",
    "        # TFLAG[tstep_idx] from the dispersion file named with CDATETIME to represent current_hour in our final IDX file\n",
    "        if found:\n",
    "            # the number of hours difference between 0th TFLAG and current_hour is tstep_idx\n",
    "            tflag_0 = parse_tflag(most_recent_row['TFLAG'][0])\n",
    "            tstep_idx = int((current_hour - tflag_0).total_seconds() / 3600)\n",
    "            if verbose:\n",
    "                print(f'Found data for current_hour {current_hour}: using CDATETIME={most_recent_row[\"CDATETIME\"]}')\n",
    "                print(f\"tflag_0 = {tflag_0}, current_hour = {current_hour}, TFLAG[tstep_idx] = {parse_tflag(most_recent_row['TFLAG'][tstep_idx])}\")\n",
    "            \n",
    "            logging.info(f\"current_hour: {current_hour} -> CDATETIME={most_recent_row['CDATETIME']}, TFLAG[{tstep_idx}] = {most_recent_row['TFLAG'][tstep_idx]}\")\n",
    "\n",
    "            # update idx call array\n",
    "            update_idx_calls(idx_calls, most_recent_row['CDATETIME'], tstep_idx)\n",
    "        else:\n",
    "            logging.info(f\"current_hour: {current_hour} -> No available file found.\")\n",
    "            if verbose:\n",
    "                print(f'WARNING: No available file found for current_hour {current_hour} (date: {current_date}). Skipping.')\n",
    "\n",
    "        # move to next hour\n",
    "        current_hour += datetime.timedelta(hours=1)\n",
    "\n",
    "        if verbose:\n",
    "            print('~~~~~')\n",
    "\n",
    "    # move to the next day\n",
    "    current_date += datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe5512b-2bae-4fbf-9792-f798ebefa2c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # just to view in txt\n",
    "# %%capture captured_output\n",
    "# for c in idx_calls:\n",
    "#     print(c)\n",
    "\n",
    "# with open('idx_calls_v5.txt', 'w') as f:\n",
    "#     f.write(captured_output.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad0d93-02c2-4795-ae4a-0966225cfab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save idx_calls to pickle file\n",
    "with open('idx_calls_v5_parallel.pkl', 'wb') as f:\n",
    "    pickle.dump(idx_calls, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd9ca5-ca24-4f90-afc5-b61d0c3b97e3",
   "metadata": {},
   "source": [
    "## Do conversion from netCDF files to IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e63f9-b881-488b-a7cb-bfa7ccf9872e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create idx file of i'th dataset\n",
    "# useful for dealing with fields that are not all the same size:\n",
    "# https://github.com/sci-visus/OpenVisus/blob/master/Samples/jupyter/nasa_conversion_example.ipynb\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "# create OpenVisus field for the pm25 variable\n",
    "f = Field('PM25', 'float32')\n",
    "\n",
    "# create the idx file for this dataset using field f\n",
    "# dims is maximum array size, we will resample data accordingly to fit this\n",
    "# time is number of files * 24 (hours)\n",
    "db = CreateIdx(url=idx_dir + '/firesmoke.idx', fields=[f], \n",
    "               dims=[int(max_grid['COL']), int(max_grid['ROW'])], time=[0, len(idx_calls) - 1, '%00000000d/'])\n",
    "\n",
    "# threshold to use to change small-enough resampled values to 0\n",
    "thresh = 1e-15\n",
    "\n",
    "# Function to process a single call; returns result for later writing\n",
    "def process_call(args):\n",
    "    call, firesmoke_dir, max_grid, sml_tups, big_tups, big_lat, big_lon, thresh = args\n",
    "    file_name = call[0]\n",
    "    timestamp = call[1]\n",
    "    tstep_index = call[2]\n",
    "    try:\n",
    "        ds = xr.open_dataset(f'{firesmoke_dir}/{file_name}')\n",
    "        file_vals = np.squeeze(ds['PM25'].values)\n",
    "        resamp = ds.XORIG != max_grid['XORIG']\n",
    "        if resamp:\n",
    "            file_vals_resamp = griddata(sml_tups, file_vals[tstep_index].flatten(), big_tups, method='cubic', fill_value=0)\n",
    "            file_vals_resamp[file_vals_resamp < thresh] = 0\n",
    "            file_vals_resamp = file_vals_resamp.reshape((len(big_lat), len(big_lon)))\n",
    "            result_data = file_vals_resamp.astype(np.float32)\n",
    "        else:\n",
    "            result_data = file_vals[tstep_index]\n",
    "        return (timestamp, result_data, resamp)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "        return (timestamp, None, None)\n",
    "\n",
    "# Using ThreadPoolExecutor or ProcessPoolExecutor as appropriate\n",
    "# Writing to IDX must be sequential to keep timestep order\n",
    "results = [None] * len(idx_calls)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Map futures to idx_call indices to preserve ordering for sequential write\n",
    "    future_to_idx = {executor.submit(process_call, (call, firesmoke_dir, max_grid, sml_tups, big_tups, big_lat, big_lon, thresh)): i \n",
    "                     for i, call in enumerate(idx_calls)}\n",
    "    for future in tqdm(concurrent.futures.as_completed(future_to_idx), total=len(idx_calls)):\n",
    "        idx = future_to_idx[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results[idx] = result # maintain correct time order\n",
    "        except Exception as exc:\n",
    "            print(f'Call at idx {idx} generated an exception: {exc}')\n",
    "\n",
    "# Now, sequentially write outputs to db in order\n",
    "for tstep, (timestamp, data, resamp) in enumerate(results):\n",
    "    if data is None:\n",
    "        print(f\"Skipping tstep {tstep} due to error.\")\n",
    "        continue\n",
    "    db.write(data=data, field=f, time=tstep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b908337-8454-4f24-9cec-4325ad74f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to idx data directory\n",
    "os.chdir(idx_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3af4da-d469-44b9-9c90-20216304c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress dataset\n",
    "db.compressDataset(['zip'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wired_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
